{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Policy-based Reinforcement Learning\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-53439-ep-2025-students/refs/heads/main/assets/logo.jpg?raw=true\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "[CSC_53439_EP-2025](https://moodle.ip-paris.fr/course/view.php?id=10716) Lab session #3\n",
    "\n",
    "2019-2025 Jérémie Decock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jeremiedecock/polytechnique-csc-53439-ep-2025-students/blob/main/lab3_mcts.ipynb)\n",
    "\n",
    "[![My Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jeremiedecock/polytechnique-csc-53439-ep-2025-students/main?filepath=lab3_mcts.ipynb)\n",
    "\n",
    "[![NbViewer](https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg)](https://nbviewer.jupyter.org/github/jeremiedecock/polytechnique-csc-53439-ep-2025-students/blob/main/lab3_mcts.ipynb)\n",
    "\n",
    "[![Local](https://img.shields.io/badge/Local-Save%20As...-blue)](https://github.com/jeremiedecock/polytechnique-csc-53439-ep-2025-students/raw/main/lab3_mcts.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In our preceding labs, we've delved into model-free approaches, encompassing both value-based and policy-based methods.\n",
    "\n",
    "Model-free reinforcement learning methods learn directly from episodes of experience. They are called \"model-free\" because they do not need to know or learn the model of the environment (i.e., the reward and transition probability functions). Examples include Q-learning and policy gradients.\n",
    "\n",
    "On the other hand, model-based reinforcement learning methods attempt to first learn a model of the environment (i.e., the reward and transition probability functions), and then use this model to make decisions. They can plan ahead by simulating future states in the model, which can lead to more efficient learning than model-free methods.\n",
    "\n",
    "The aim of this lab is to provide an in-depth exploration of one of the most famous model-based reinforcement learning method: *Monte Carlo Tree Search (MCTS)*.\n",
    "We will focus on the planning aspect of model-based reinforcement learning (using MCTS), so we provide an internal model (*NaughtsAndCrossesState* and *NaughtsAndCrossesAction*) but in many practical cases, the model is not available and must be learned from experience.\n",
    "\n",
    "In this Python notebook, you will implement and assess this algorithm on the Naughts and Crosses board game.\n",
    "\n",
    "You can either:\n",
    "- open, edit and execute the notebook in *Google Colab* following this link: https://colab.research.google.com/github/jeremiedecock/polytechnique-csc-53439-ep-2025-students/blob/main/lab3_mcts.ipynb ; this is the **recommended** choice as you have nothing to install on your computer\n",
    "- open, edit and execute the notebook in *MyBinder* (if for any reason the Google Colab solution doesn't work): https://mybinder.org/v2/gh/jeremiedecock/polytechnique-csc-53439-ep-2025-students/main?filepath=lab3_mcts.ipynb\n",
    "- download, edit and execute the notebook on your computer if Python3 and JypyterLab are already installed: https://github.com/jeremiedecock/polytechnique-csc-53439-ep-2025-students/raw/main/lab3_mcts.ipynb\n",
    "\n",
    "If you work with Google Colab or MyBinder, **remember to save or download your work regularly or you may lose it!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Python environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: This notebook relies on the graphviz tool.\n",
    "\n",
    "If you are using Google Colab, graphviz is already installed.\n",
    "Otherwise, check https://graphviz.org/.\n",
    "\n",
    "For instance, on Debian Gnu/Linux, Ubuntu, ..., you can install it with the following command:\n",
    "```bash\n",
    "apt install graphviz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! apt install graphviz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
