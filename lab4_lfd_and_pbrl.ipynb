{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning from Demonstrations\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-53439-ep-2025-students/refs/heads/main/assets/logo.jpg?raw=true\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "[CSC_53439_EP-2025](https://moodle.ip-paris.fr/course/view.php?id=10716) Lab session #4\n",
    "\n",
    "2019-2025 Jérémie Decock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jeremiedecock/polytechnique-csc-53439-ep-2025-students/blob/main/lab4_lfd_and_pbrl.ipynb)\n",
    "\n",
    "[![My Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jeremiedecock/polytechnique-csc-53439-ep-2025-students/main?filepath=lab4_lfd_and_pbrl.ipynb)\n",
    "\n",
    "[![NbViewer](https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg)](https://nbviewer.jupyter.org/github/jeremiedecock/polytechnique-csc-53439-ep-2025-students/blob/main/lab4_lfd_and_pbrl.ipynb)\n",
    "\n",
    "[![Local](https://img.shields.io/badge/Local-Save%20As...-blue)](https://github.com/jeremiedecock/polytechnique-csc-53439-ep-2025-students/raw/main/lab4_lfd_and_pbrl.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The purpose of this lab is to introduce some classic algorithms of *Learning from Demonstrations*. We will see how they work, their caveats and benefits.\n",
    "\n",
    "*Learning from Demonstrations* (LfD) is an approach in reinforcement learning where an agent learns behaviors by observing examples provided by a demonstrator. This field is divided into two main branches: *Imitation Learning* and *Inverse Reinforcement Learning* (IRL).\n",
    "\n",
    "- **Imitation Learning** involves directly mimicking the demonstrator's actions. The agent learns to replicate the demonstrated behavior without attempting to infer the underlying objectives or rewards driving the actions. It focuses on reproducing successful behaviors in a given task, often through supervised learning.\n",
    "\n",
    "- **Inverse Reinforcement Learning (IRL)**, on the other hand, seeks to infer the demonstrator's underlying reward function. Instead of copying actions, the agent tries to discover the goals or preferences that motivated the demonstrator’s behavior. Once the reward function is learned, the agent can optimize its own policy to achieve similar outcomes.\n",
    "\n",
    "The main objective of *Learning from Demonstrations* is to enable agents to learn effectively in settings where the reward function is unknown or poorly defined, by leveraging expert demonstrations as a source of supervisory signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both parts of this lab will focus on *Imitation Learning*. In the first part, we will implement the *Behavioral Cloning* algorithm, while the second part will introduce the *GAIL* algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can either:\n",
    "- open, edit and execute the notebook in *Google Colab* following this link: https://colab.research.google.com/github/jeremiedecock/polytechnique-csc-53439-ep-2025-students/blob/main/lab4_lfd_and_pbrl.ipynb ; this is the **recommended** choice as you have nothing to install on your computer\n",
    "- open, edit and execute the notebook in *MyBinder* (if for any reason the Google Colab solution doesn't work): https://mybinder.org/v2/gh/jeremiedecock/polytechnique-csc-53439-ep-2025-students/main?filepath=lab4_lfd_and_pbrl.ipynb\n",
    "- download, edit and execute the notebook on your computer if Python3 and JypyterLab are already installed: https://github.com/jeremiedecock/polytechnique-csc-53439-ep-2025-students/raw/main/lab4_lfd_and_pbrl.ipynb\n",
    "\n",
    "If you work with Google Colab or MyBinder, **remember to save or download your work regularly or you may lose it!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Submission\n",
    "\n",
    "Please submit your completed notebook in [Moodle : \"Lab 4 - Submission\"](https://moodle.ip-paris.fr/mod/assign/view.php?id=185691).\n",
    "\n",
    "### Submission Guidelines\n",
    "\n",
    "1. **File Naming:** Rename your notebook as follows: **`firstname_lastname-04.ipynb`** where `firstname` and `lastname` match your email address. *Example: `jesse_read-04.ipynb`*\n",
    "2. **Clear Output Cells:** To reduce file size (**must be under 500 KB**), clear all output cells before submitting. This includes rendered images, videos, plots, and dataframes...\n",
    "   - **JupyterLab:**\n",
    "     - Click **\"Kernel\" → \"Restart Kernel and Clear Outputs of All Cells...\"**\n",
    "     - Then go to **\"File\" → \"Save Notebook As...\"**\n",
    "   - **Google Colab:**\n",
    "     - Click **\"Edit\" → \"Clear all outputs\"**\n",
    "     - Then go to **\"File\" → \"Download\" → \"Download.ipynb\"**\n",
    "   - **VSCode:**\n",
    "     - Click **\"Clear All Outputs\"**\n",
    "     - Then **save your file**\n",
    "3. **Upload Your File:** Only **`.ipynb`** files are accepted.\n",
    "\n",
    "**Note:** Bonus parts (if any) are optional, as their name suggests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Python environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook relies on several libraries including `torch`, `gymnasium`, `numpy`, `pandas`, `seaborn`, `imageio`, `pygame`, and `tqdm`.\n",
    "A complete list of dependencies can be found in the following [requirements_lab4.txt](https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-53439-ep-2025-students/main/requirements_lab4.txt) file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you use Google Colab\n",
    "\n",
    "If you use Google Colab, execute the next cell to install required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, subprocess\n",
    "\n",
    "def is_colab():\n",
    "    return \"google.colab\" in sys.modules\n",
    "\n",
    "def run_subprocess_command(cmd):\n",
    "    # run the command\n",
    "    process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n",
    "    # print the output\n",
    "    for line in process.stdout:\n",
    "        print(line.decode().strip())\n",
    "\n",
    "if is_colab():\n",
    "    run_subprocess_command(\"apt install swig\")\n",
    "    run_subprocess_command(\"pip install -r https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-53439-ep-2025-students/main/requirements_lab4_google_colab.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you have downloaded the notebook on your computer and execute it in your own Python environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up the necessary dependencies, run the following commands to establish a [Python virtual environment (venv)](https://docs.python.org/3/library/venv.html) that includes all the essential libraries for this lab.\n",
    "\n",
    "#### On Posix systems (Linux, MacOSX, WSL, ...)\n",
    "\n",
    "```bash\n",
    "python3 -m venv env-lab4\n",
    "source env-lab4/bin/activate\n",
    "python3 -m pip install --upgrade pip\n",
    "python3 -m pip install -r https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-53439-ep-2025-students/main/requirements_lab4.txt\n",
    "```\n",
    "\n",
    "#### On Windows\n",
    "\n",
    "```bash\n",
    "python3 -m venv env-lab4\n",
    "env-lab4\\Scripts\\activate.bat\n",
    "python3 -m pip install --upgrade pip\n",
    "python3 -m pip install -r https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-53439-ep-2025-students/main/requirements_lab4.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run CSC-53439-EP notebooks locally in a dedicated Docker container\n",
    "\n",
    "If you are familiar with Docker (or Podman), an image is available on Docker Hub for this lab:\n",
    "\n",
    "```bash\n",
    "docker run -it --rm --user root -p 8888:8888 -e NB_UID=$(id -u) -e NB_GID=$(id -g) -v \"${PWD}\":/home/jovyan/work jdhp/csc-53439-ep-lab4:latest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from IPython.display import Video\n",
    "import json\n",
    "import lzma\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from typing import List, Tuple, Deque, Optional, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGS_DIR = Path(\"figs/\")       # Where to save figures (.gif files)\n",
    "PLOTS_DIR = Path(\"figs/\")      # Where to save plots (.png or .svg files)\n",
    "MODELS_DIR = Path(\"models/\")   # Where to save models (.pth files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not FIGS_DIR.exists():\n",
    "    FIGS_DIR.mkdir()\n",
    "if not PLOTS_DIR.exists():\n",
    "    PLOTS_DIR.mkdir()\n",
    "if not MODELS_DIR.exists():\n",
    "    MODELS_DIR.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch can run on both CPUs and GPUs. The following cell will determine the device PyTorch will use. If a GPU is available, PyTorch will use it; otherwise, it will use the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Set the device to CUDA if available, otherwise use CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For utilizing a GPU on Google Colab, you also have to activate it following the steps outlined [here](https://colab.research.google.com/notebooks/gpu.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available GPUs:\")\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"- Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"- No GPU available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a very recent GPU and want to use it, you might need to install a specific version of PyTorch compatible with your Cuda version.\n",
    "For this, you will have to edit the [requirements_lab4.txt](https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-53439-ep-2025-students/main/requirements_lab4.txt) file and replace the current version of PyTorch with the one compatible with your Cuda version.\n",
    "Check the [official PyTorch website](https://pytorch.org/get-started/locally/) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the GPU is not very useful for CartPole (but useful for MuJoCo) because CartPole is a simple and quick problem to solve, and CUDA spends more time transferring data between the CPU and GPU than processing it directly on the CPU.\n",
    "\n",
    "You can uncomment the next cell to explicitly instruct PyTorch to train neural networks using the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PyTorch will train and test neural networks on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Behavioral Cloning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Hands on MountainCar environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MountainCar is a classic reinforcement learning environment. In this simple 2D scenario, an underpowered car must navigate a hill, but it lacks the power to ascend directly. Instead, the car must learn to use the hill's slopes to build momentum and ultimately reach the flag at the top. While the environment is straightforward, it becomes interesting due to its sparse reward signal, making it an excellent candidate for learning from demonstrations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1:** refer to the following link [MountainCar Environment](https://gymnasium.farama.org/environments/classic_control/mountain_car/) to familiarize yourself with the MountainCar environment if you are not already."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some information about the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0', render_mode=\"rgb_array\")\n",
    "\n",
    "mountain_car_state_dim = env.observation_space.shape[0]\n",
    "mountain_car_action_dim = env.action_space.n.item()\n",
    "\n",
    "print(f\"State space size is: { env.observation_space }\")\n",
    "print(f\"Action space size is: { env.action_space }\")\n",
    "print(\"Actions are: {\" + \", \".join([str(a) for a in range(env.action_space.n)]) + \"}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:** Run the following cells and check different basic \n",
    "policies (for instance constant actions or randomly drawn actions) to discover the MountainCar environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Test the MountainCar environment with a constant policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_DIRNAME = \"lab4_mountain-car_action0\"\n",
    "\n",
    "env = gym.make('MountainCar-v0', render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(env, FIGS_DIR / VIDEO_DIRNAME)\n",
    "\n",
    "observation, info = env.reset()\n",
    "done = False\n",
    "\n",
    "for t in range(200):\n",
    "    action = 0\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "env.close()\n",
    "\n",
    "Video(FIGS_DIR / VIDEO_DIRNAME / \"rl-video-episode-0.mp4\", embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_DIRNAME = \"lab4_mountain-car_action1\"\n",
    "\n",
    "env = gym.make('MountainCar-v0', render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(env, FIGS_DIR / VIDEO_DIRNAME)\n",
    "\n",
    "observation, info = env.reset()\n",
    "done = False\n",
    "\n",
    "for t in range(200):\n",
    "    action = 1\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "env.close()\n",
    "\n",
    "Video(FIGS_DIR / VIDEO_DIRNAME / \"rl-video-episode-0.mp4\", embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_DIRNAME = \"lab4_mountain-car_action2\"\n",
    "\n",
    "env = gym.make('MountainCar-v0', render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(env, FIGS_DIR / VIDEO_DIRNAME)\n",
    "\n",
    "observation, info = env.reset()\n",
    "done = False\n",
    "\n",
    "for t in range(200):\n",
    "    action = 2\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "env.close()\n",
    "\n",
    "Video(FIGS_DIR / VIDEO_DIRNAME / \"rl-video-episode-0.mp4\", embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the MountainCar environment with a random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_DIRNAME = \"lab4_mountain-car_random_action\"\n",
    "\n",
    "env = gym.make('MountainCar-v0', render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(env, FIGS_DIR / VIDEO_DIRNAME)\n",
    "\n",
    "observation, info = env.reset()\n",
    "done = False\n",
    "\n",
    "for t in range(200):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "env.close()\n",
    "\n",
    "Video(FIGS_DIR / VIDEO_DIRNAME / \"rl-video-episode-0.mp4\", embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the MountainCar environment with a good handcrafted policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3:** The MountainCar environment is simple in design but poses a significant challenge for many algorithms, such as PPO, due to its sparse reward structure. The agent must engage in extensive exploration before receiving its first positive reward, which occurs only when it successfully reaches the flag at the top of the hill. Despite this, the task can be solved with a surprisingly simple policy. Can you discover it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_DIRNAME = \"lab4_mountain-car_random_action\"\n",
    "\n",
    "env = gym.make('MountainCar-v0', render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(env, FIGS_DIR / VIDEO_DIRNAME)\n",
    "\n",
    "observation, info = env.reset()\n",
    "done = False\n",
    "\n",
    "for t in range(200):\n",
    "\n",
    "    # TODO...\n",
    "\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "env.close()\n",
    "\n",
    "Video(FIGS_DIR / VIDEO_DIRNAME / \"rl-video-episode-0.mp4\", embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behavioral Cloning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Behavioral Cloning* ([D. A. Pomerleau, *Efficient Training of Artificial Neural Networks for Autonomous Navigation*, Neural Computation, vol. 3, no. 1, pp. 88–97, 1991](https://cours.etsmtl.ca/sys843/REFS/ORG/pomerleau_alvinn.pdf)) is one of the most fundamental approaches to *Imitation Learning*. The concept is straightforward: an *expert* provides high-quality traces, or demonstrations, and the learning agent's task is to mimic the expert’s behavior.\n",
    "In *Imitation Learning*, *traces* or *demonstrations* refer to sequences of state-action pairs generated by an expert while performing a task. These demonstrations serve as examples for the agent to learn from. Each demonstration consists of a series of observations (states) encountered by the expert, along with the corresponding actions taken in those states.\n",
    "For example, in a driving task, a demonstration might be a series of snapshots of the environment (such as the car’s position and speed) and the actions the expert driver took at each moment (such as steering or braking). These state-action pairs are recorded and used to train the agent, enabling it to learn how to behave similarly in similar situations.\n",
    "\n",
    "The goal of *Behavioral Cloning* is to map states to the actions the expert would take, essentially allowing the agent to \"clone\" the expert's behavior. The quality and variety of these demonstrations are critical for successful learning, as they provide the agent with the knowledge it needs to act appropriately across different scenarios.\n",
    "Typically, expert demonstrations are obtained by recording human behavior, which is then used to train the agent.\n",
    "\n",
    "**Note**: In the original publication, the algorithm learns a stochastic policy by maximizing the likelihood of the expert's actions. However, in this lab, we will train a deterministic policy by minimizing the Categorical Cross Entropy between the expert's actions and the model’s predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Behavioral Cloning on MountainCar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a PyTorch dataset from the demonstrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download expert demonstrations\n",
    "\n",
    "The expert demonstrations are available at the following URL: https://github.com/jeremiedecock/polytechnique-csc-53439-ep-2025-students/raw/refs/heads/main/models/lab4_expert_mountaincar-v0_handcrafted/demonstrations.json.xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p models/lab4_expert_mountaincar-v0_handcrafted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/jeremiedecock/polytechnique-csc-53439-ep-2025-students/raw/refs/heads/main/models/lab4_expert_mountaincar-v0_handcrafted/demonstrations.json.xz -O models/lab4_expert_mountaincar-v0_handcrafted/demonstrations.json.xz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpertDemonstrationsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, json_demonstrations_file_path, transform=None, target_transform=None):\n",
    "        super().__init__()\n",
    "        self.json_demonstrations_file_path = json_demonstrations_file_path\n",
    "\n",
    "        with lzma.open(self.json_demonstrations_file_path, \"rt\") as f:\n",
    "            demonstrations_list = json.load(f)\n",
    "\n",
    "        self._observations_tensor = torch.tensor([transition[\"observation\"] for transition in demonstrations_list], dtype=torch.float32)\n",
    "        self._actions_tensor = torch.tensor([transition[\"action\"] for transition in demonstrations_list], dtype=torch.long)\n",
    "\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._observations_tensor)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        observation = self._observations_tensor[idx]\n",
    "        action = self._actions_tensor[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            observation = self.transform(observation)\n",
    "        if self.target_transform:\n",
    "            action = self.target_transform(action)\n",
    "\n",
    "        return observation, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mountain_car_expert_dataset = ExpertDemonstrationsDataset(Path(\"models\") / \"lab4_expert_mountaincar-v0_handcrafted\" / \"demonstrations.json.xz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task1**: Take time to check the definition and the content of this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_policy_df = pd.DataFrame([{\"s_1\": observation_tensor[0], \"s_2\": observation_tensor[1], \"a\": action_tensor} for observation_tensor, action_tensor in mountain_car_expert_dataset])\n",
    "expert_policy_df.plot(kind=\"scatter\", x=\"s_1\", y=\"s_2\", c=\"a\", colormap=\"viridis\", colorbar=True, figsize=(10, 7), s=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task2**: Implement a neural network that takes the state as input and outputs the action. The neural network should have the following architecture:\n",
    "- A first fully connected layer with `hidden_units` units and ReLU activation function\n",
    "- A second fully connected layer with `n_actions` units and no activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscretePolicyNetwork(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations: int, n_actions: int, hidden_units: int):\n",
    "\n",
    "        super(DiscretePolicyNetwork, self).__init__()\n",
    "\n",
    "        # TODO...\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        # TODO...\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mountain_car_model = DiscretePolicyNetwork(n_observations=mountain_car_state_dim, n_actions=mountain_car_action_dim, hidden_units=8).to(device)\n",
    "print(mountain_car_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task3**: Implement a classical supervised learning training function that train one epoch of the neural network on the dataset. This function will be called at each epoch in the *Train the model* cell defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, verbose=True):\n",
    "\n",
    "    # TODO..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the testing loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task4**: Implement a classical supervised learning testing function that assess the performance of the neural network on the test dataset at each epoch. This function will be called at each epoch in the *Train the model* cell defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "\n",
    "    # TODO..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we will split the dataset (expert demonstrations) into a training and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(mountain_car_expert_dataset))\n",
    "test_size = len(mountain_car_expert_dataset) - train_size\n",
    "\n",
    "train_subset, test_subset = torch.utils.data.random_split(mountain_car_expert_dataset, [train_size, test_size])\n",
    "\n",
    "mountain_car_train_dataloader = torch.utils.data.DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "mountain_car_test_dataloader = torch.utils.data.DataLoader(test_subset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the loss function and the optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task5**: Define the loss function used to train the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mountain_car_loss_fn = # TODO...\n",
    "\n",
    "mountain_car_optimizer = torch.optim.Adam(mountain_car_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\\n-------------------------------\")\n",
    "\n",
    "    train(mountain_car_train_dataloader, mountain_car_model, mountain_car_loss_fn, mountain_car_optimizer, verbose=False)\n",
    "    test(mountain_car_test_dataloader, mountain_car_model, mountain_car_loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot the learned policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now compare the learned policy with the expert's policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expert_policy_df = pd.DataFrame([{\"s_1\": test_subset[idx][0][0], \"s_2\": test_subset[idx][0][1], \"a\": test_subset[idx][1]} for idx in range(len(test_subset))])\n",
    "# expert_policy_df.plot(kind=\"scatter\", x=\"s_1\", y=\"s_2\", c=\"a\", colormap=\"viridis\", colorbar=True, figsize=(10, 7), s=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_policy_df = pd.DataFrame([{\"s_1\": test_subset[idx][0][0], \"s_2\": test_subset[idx][0][1], \"a\": mountain_car_model(test_subset[idx][0]).argmax().item()} for idx in range(len(test_subset))])\n",
    "expert_policy_df.plot(kind=\"scatter\", x=\"s_1\", y=\"s_2\", c=\"a\", colormap=\"viridis\", colorbar=True, figsize=(10, 7), s=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the learned policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_DIRNAME = \"lab4_mountain-car_trained_policy\"\n",
    "\n",
    "env = gym.make('MountainCar-v0', render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(env, FIGS_DIR / VIDEO_DIRNAME)\n",
    "\n",
    "observation, info = env.reset()\n",
    "done = False\n",
    "\n",
    "for t in range(200):\n",
    "    observation_tensor = torch.tensor(observation, dtype=torch.float32).to(device)\n",
    "    action = mountain_car_model(observation_tensor).argmax().item()\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "env.close()\n",
    "\n",
    "Video(FIGS_DIR / VIDEO_DIRNAME / \"rl-video-episode-0.mp4\", embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Hands on LunarLander environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will apply the *Behavioral Cloning* algorithm to the LunarLander environment. The LunarLander environment is a classic reinforcement learning task where an agent must learn to land a spacecraft safely on the moon. The agent controls the spacecraft's engines, which can fire in four directions: do nothing, fire left, fire right, or fire both engines downward. The agent receives a reward for successfully landing the spacecraft and a penalty for crashing or running out of fuel.\n",
    "\n",
    "The reward signal is less sparse than in the MountainCar environment, but the dynamics is more complex making it interesting too for *Imitation Learning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1:** refer to the following link [LunarLander-v3 Environment](https://gymnasium.farama.org/environments/box2d/lunar_lander/) to familiarize yourself with the LunarLander-v3 environment if you are not already."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some information about the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v3', render_mode=\"rgb_array\")\n",
    "\n",
    "lunar_lander_state_dim = env.observation_space.shape[0]\n",
    "lunar_lander_action_dim = env.action_space.n.item()\n",
    "\n",
    "print(f\"State space size is: { env.observation_space }\")\n",
    "print(f\"Action space size is: { env.action_space }\")\n",
    "print(\"Actions are: {\" + \", \".join([str(a) for a in range(env.action_space.n)]) + \"}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:** Run the following cells and check different basic \n",
    "policies (for instance constant actions or randomly drawn actions) to discover the LunarLander-v3 environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the LunarLander environment with a constant policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_DIRNAME = \"lab4_lunar-lander_action0\"\n",
    "\n",
    "env = gym.make('LunarLander-v3', render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(env, FIGS_DIR / VIDEO_DIRNAME)\n",
    "\n",
    "observation, info = env.reset()\n",
    "done = False\n",
    "\n",
    "for t in range(200):\n",
    "    action = 0\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "env.close()\n",
    "\n",
    "Video(FIGS_DIR / VIDEO_DIRNAME / \"rl-video-episode-0.mp4\", embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_DIRNAME = \"lab4_lunar-lander_action1\"\n",
    "\n",
    "env = gym.make('LunarLander-v3', render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(env, FIGS_DIR / VIDEO_DIRNAME)\n",
    "\n",
    "observation, info = env.reset()\n",
    "done = False\n",
    "\n",
    "for t in range(200):\n",
    "    action = 1\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "env.close()\n",
    "\n",
    "Video(FIGS_DIR / VIDEO_DIRNAME / \"rl-video-episode-0.mp4\", embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_DIRNAME = \"lab4_lunar-lander_action2\"\n",
    "\n",
    "env = gym.make('LunarLander-v3', render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(env, FIGS_DIR / VIDEO_DIRNAME)\n",
    "\n",
    "observation, info = env.reset()\n",
    "done = False\n",
    "\n",
    "for t in range(200):\n",
    "    action = 2\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "env.close()\n",
    "\n",
    "Video(FIGS_DIR / VIDEO_DIRNAME / \"rl-video-episode-0.mp4\", embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_DIRNAME = \"lab4_lunar-lander_action3\"\n",
    "\n",
    "env = gym.make('LunarLander-v3', render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(env, FIGS_DIR / VIDEO_DIRNAME)\n",
    "\n",
    "observation, info = env.reset()\n",
    "done = False\n",
    "\n",
    "for t in range(200):\n",
    "    action = 3\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "env.close()\n",
    "\n",
    "Video(FIGS_DIR / VIDEO_DIRNAME / \"rl-video-episode-0.mp4\", embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the LunarLander environment with a random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_DIRNAME = \"lab4_lunar-lander_random_action\"\n",
    "\n",
    "env = gym.make('LunarLander-v3', render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(env, FIGS_DIR / VIDEO_DIRNAME)\n",
    "\n",
    "observation, info = env.reset()\n",
    "done = False\n",
    "\n",
    "for t in range(200):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "env.close()\n",
    "\n",
    "Video(FIGS_DIR / VIDEO_DIRNAME / \"rl-video-episode-0.mp4\", embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Behavioral Cloning on LunarLander"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will reuse most of the code from the previous exercise to apply the *Behavioral Cloning* algorithm to the LunarLander environment.\n",
    "There are very little to do except check that the code of the previous exercise is still working on the LunarLander environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a PyTorch dataset from the demonstrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download expert demonstrations\n",
    "\n",
    "The expert demonstrations are available at the following URL: https://github.com/jeremiedecock/polytechnique-csc-53439-ep-2025-students/raw/refs/heads/main/models/lab4_expert_lunar-lander-v2-discrete-nowind_ppo/demonstrations.json.xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p models/lab4_expert_lunar-lander-v2-discrete-nowind_ppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/jeremiedecock/polytechnique-csc-53439-ep-2025-students/raw/refs/heads/main/models/lab4_expert_lunar-lander-v2-discrete-nowind_ppo/demonstrations.json.xz -O models/lab4_expert_lunar-lander-v2-discrete-nowind_ppo/demonstrations.json.xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lunar_lander_expert_dataset = ExpertDemonstrationsDataset(Path(\"models\") / \"lab4_expert_lunar-lander-v2-discrete-nowind_ppo\" / \"demonstrations.json.xz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lunar_lander_model = DiscretePolicyNetwork(n_observations=lunar_lander_state_dim, n_actions=lunar_lander_action_dim, hidden_units=64).to(device)\n",
    "print(lunar_lander_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(lunar_lander_expert_dataset))\n",
    "test_size = len(lunar_lander_expert_dataset) - train_size\n",
    "\n",
    "train_subset, test_subset = torch.utils.data.random_split(lunar_lander_expert_dataset, [train_size, test_size])\n",
    "\n",
    "lunar_lander_train_dataloader = torch.utils.data.DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "lunar_lander_test_dataloader = torch.utils.data.DataLoader(test_subset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the loss function and the optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task1**: Define the loss function used to train the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lunar_lander_loss_fn = # TODO...\n",
    "\n",
    "lunar_lander_optimizer = torch.optim.Adam(lunar_lander_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\\n-------------------------------\")\n",
    "\n",
    "    train(lunar_lander_train_dataloader, lunar_lander_model, lunar_lander_loss_fn, lunar_lander_optimizer, verbose=False)\n",
    "    test(lunar_lander_test_dataloader, lunar_lander_model, lunar_lander_loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the learned policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_DIRNAME = \"lab4_lunar-lander_trained_policy\"\n",
    "\n",
    "env = gym.make('LunarLander-v3', render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(env, FIGS_DIR / VIDEO_DIRNAME)\n",
    "\n",
    "observation, info = env.reset()\n",
    "done = False\n",
    "\n",
    "for t in range(200):\n",
    "    observation_tensor = torch.tensor(observation, dtype=torch.float32).to(device)\n",
    "    action = lunar_lander_model(observation_tensor).argmax().item()\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "env.close()\n",
    "\n",
    "Video(FIGS_DIR / VIDEO_DIRNAME / \"rl-video-episode-0.mp4\", embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: GAIL (bonus part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to GAIL\n",
    "\n",
    "**Generative Adversarial Imitation Learning (GAIL)** ([Ho & Ermon, 2016](https://proceedings.neurips.cc/paper_files/paper/2016/file/cc7e2b878868cbae992d1fb743995d8f-Paper.pdf)) is an advanced imitation learning algorithm that combines ideas from Generative Adversarial Networks (GANs) and Inverse Reinforcement Learning (IRL).\n",
    "\n",
    "Unlike Behavioral Cloning, which directly learns to map states to actions through supervised learning, GAIL learns both a policy and a reward function simultaneously:\n",
    "\n",
    "1. **Discriminator**: A neural network that tries to distinguish between state-action pairs from expert demonstrations and those generated by the current policy\n",
    "2. **Policy (Generator)**: A neural network that tries to generate trajectories that fool the discriminator into thinking they come from the expert\n",
    "\n",
    "The key insight is that the discriminator provides a learned reward signal to train the policy using reinforcement learning (typically PPO). This approach addresses some limitations of behavioral cloning:\n",
    "- It can handle distribution mismatch between training and test states\n",
    "- It doesn't require explicit reward functions\n",
    "- It's more robust to imperfect demonstrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: GAIL on MountainCar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: This implementation of GAIL is intentionally kept simple to focus on understanding the core algorithm without the complexity of advanced policy training procedures. We use the **REINFORCE** algorithm (vanilla policy gradient) to train the policy, which makes the learning process transparent and easier to follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the policy network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task1**: Implement a neural network that takes the state as input and outputs the action. The neural network should have the following architecture:\n",
    "- Input Layer:\n",
    "  - The network takes an input with a dimension of obs_dim.\n",
    "- Hidden Layer:\n",
    "  - The first hidden layer is a fully connected (Linear) layer with 128 units.\n",
    "  - This is followed by a ReLU activation function.\n",
    "- Output Layer:\n",
    "  - The output layer is a fully connected (Linear) layer with act_dim units.\n",
    "  - This is followed by a Softmax activation function, which ensures that the output is a probability distribution over actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(torch.nn.Module):\n",
    "    \"\"\"Policy network for REINFORCE training in GAIL\"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "\n",
    "        # TODO...\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the policy network.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            State tensor of shape (n_observations)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Action probabilities, shape (n_actions)\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO...\n",
    "\n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the discriminator neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task2**: Implement a neural network that takes the state-action pair as input and outputs the probability that the pair comes from the expert policy. The neural network should have the following architecture:\n",
    "- Input Layer:\n",
    "  - The network takes a concatenated input of observations and actions with a combined dimension of obs_dim + act_dim.\n",
    "- Hidden Layer:\n",
    "  - The first hidden layer is a fully connected (Linear) layer with 128 units.\n",
    "  - This is followed by a ReLU activation function.\n",
    "- Output Layer:\n",
    "  - The output layer is a fully connected (Linear) layer with 1 unit.\n",
    "  - This is followed by a Sigmoid activation function, which outputs a probability indicating whether the input is from the expert or the generated policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(torch.nn.Module):\n",
    "    \"\"\"Discriminator network that distinguishes expert from policy trajectories\"\"\"\n",
    "\n",
    "    def __init__(self, observations_dim: int, actions_dim: int):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # TODO...\n",
    "\n",
    "    def forward(self, observations: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        # TODO...\n",
    "\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v3')\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]  # Dimensions de l'observation\n",
    "act_dim = env.action_space.n              # Nombre d'actions possibles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the policy and the discriminator networks and their optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of the networks\n",
    "policy = PolicyNetwork(obs_dim, act_dim)\n",
    "discriminator = Discriminator(obs_dim, act_dim)\n",
    "\n",
    "# Optimizers\n",
    "policy_optimizer = torch.optim.Adam(policy.parameters(), lr=1e-3)\n",
    "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load expert demonstrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_dataset = ExpertDemonstrationsDataset(Path(\"models\") / \"lab4_expert_lunar-lander-v2-discrete-nowind_ppo\" / \"demonstrations.json.xz\")\n",
    "expert_loader = torch.utils.data.DataLoader(expert_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to collect agent trajectories\n",
    "\n",
    "The following function collects the agent's trajectories using the given policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_agent_trajectories(\n",
    "    policy: PolicyNetwork,\n",
    "    env: gym.Env,\n",
    "    num_episodes: int=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Collect trajectories from the agent using the given policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    policy : PolicyNetwork\n",
    "        The neural network policy that outputs action probabilities\n",
    "    env : gym.Env\n",
    "        The gymnasium environment\n",
    "    num_episodes : int, optional\n",
    "        Number of episodes to collect (default: 10)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    trajectories : list of tuple\n",
    "        List of tuples (obs_list, action_list) for each episode, where:\n",
    "        - obs_list : list of numpy.ndarray\n",
    "            Observations collected during the episode\n",
    "        - action_list : list of int\n",
    "            Actions taken during the episode\n",
    "    \"\"\"\n",
    "    trajectories = []\n",
    "\n",
    "    # Collect multiple episodes\n",
    "    for _ in range(num_episodes):\n",
    "        obs_list = []\n",
    "        action_list = []\n",
    "\n",
    "        # Reset the environment to start a new episode\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        # Run one episode until termination\n",
    "        while not done:\n",
    "            # Convert observation to tensor for the policy network\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32)\n",
    "\n",
    "            # Get action probabilities from the policy (no gradient needed for inference)\n",
    "            with torch.no_grad():\n",
    "                action_probs = policy(obs_tensor)\n",
    "\n",
    "            # Sample an action from the probability distribution\n",
    "            action = torch.multinomial(action_probs, 1).item()\n",
    "\n",
    "            # Store the observation and action\n",
    "            obs_list.append(obs)\n",
    "            action_list.append(action)\n",
    "\n",
    "            # Take a step in the environment\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "            # Handle truncated episodes (time limit reached)\n",
    "            if truncated:\n",
    "                done = True\n",
    "\n",
    "        # Store the complete trajectory for this episode\n",
    "        trajectories.append((obs_list, action_list))\n",
    "\n",
    "    return trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The main training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task3**: Implement the main training loop for the Generative Adversarial Imitation Learning (GAIL) algorithm. Here's a step-by-step explanation:\n",
    "\n",
    "1. **Initialization**:\n",
    "   - `num_iterations`, `num_agent_episodes`, and `gamma` (discount factor) are defined.\n",
    "   - The loop runs for [`num_iterations`.\n",
    "\n",
    "2. **Collecting Agent Trajectories**:\n",
    "   - For each iteration, agent trajectories are collected using the current policy by running `num_agent_episodes` episodes in the environment.\n",
    "\n",
    "3. **Processing Trajectories for Policy Gradient**:\n",
    "   - For each trajectory, observations and actions are converted to tensors.\n",
    "   - Actions are converted to one-hot encoding.\n",
    "   - The discriminator's output is used to compute rewards for the agent.\n",
    "   - Cumulative returns are calculated using the discount factor `gamma`.\n",
    "\n",
    "4. **Concatenation of Data**:\n",
    "   - All observations, actions, and returns from the trajectories are concatenated into single tensors.\n",
    "\n",
    "5. **Policy Update**:\n",
    "   - The policy network is updated using the policy gradient method.\n",
    "   - Log probabilities of the selected actions are computed.\n",
    "   - The policy loss is calculated and backpropagated to update the policy network.\n",
    "\n",
    "6. **Data Preparation for the Discriminator**:\n",
    "   - Agent actions are converted to one-hot encoding.\n",
    "   - A batch of expert data is retrieved.\n",
    "   - Expert actions are also converted to one-hot encoding.\n",
    "\n",
    "7. **Combining and Shuffling Agent and Expert Data**:\n",
    "   - Observations and actions from both agent and expert are concatenated.\n",
    "   - Labels are created (0 for agent data, 1 for expert data).\n",
    "   - The combined data is shuffled.\n",
    "\n",
    "8. **Training the Discriminator**:\n",
    "   - The discriminator is trained to distinguish between agent and expert data.\n",
    "   - The discriminator loss is calculated using binary cross-entropy and backpropagated to update the discriminator network.\n",
    "\n",
    "9. **Logging**:\n",
    "   - Every 10 iterations, the discriminator and policy losses are printed.\n",
    "\n",
    "This loop iteratively improves the policy by making it more similar to the expert's behavior while simultaneously training the discriminator to better distinguish between agent and expert actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 500\n",
    "num_agent_episodes = 5\n",
    "gamma = 0.99  # Discount factor\n",
    "\n",
    "discriminator_loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "# Track training metrics\n",
    "training_history = {\n",
    "    'iteration': [],\n",
    "    'discriminator_loss': [],\n",
    "    'policy_loss': [],\n",
    "    # 'mean_reward': []\n",
    "}\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "\n",
    "    # Collecting agent trajectories\n",
    "    trajectories = collect_agent_trajectories(policy, env, num_episodes=num_agent_episodes)\n",
    "\n",
    "    all_observations = []\n",
    "    all_actions = []\n",
    "    all_returns = []\n",
    "\n",
    "    # ==================================\n",
    "    # STEP 1: Process agent trajectories\n",
    "    # ==================================\n",
    "\n",
    "    # Convert raw trajectories into tensors and compute returns for policy gradient\n",
    "    for transition in trajectories:\n",
    "\n",
    "        # Extract observations and actions from the trajectory\n",
    "        observation_list, action_list = transition\n",
    "\n",
    "        # --------------------\n",
    "        # Process observations\n",
    "        # --------------------\n",
    "\n",
    "        observation_tensor = torch.tensor(observation_list, dtype=torch.float32)\n",
    "        all_observations.append(observation_tensor)\n",
    "\n",
    "        # ---------------\n",
    "        # Process actions\n",
    "        # ---------------\n",
    "\n",
    "        action_tensor = torch.tensor(action_list, dtype=torch.long)\n",
    "        \n",
    "        # Convert discrete actions to one-hot encoding for the discriminator\n",
    "        action_tensor_onehot = torch.nn.functional.one_hot(action_tensor, num_classes=act_dim).float()\n",
    "\n",
    "        all_actions.append(action_tensor_onehot)\n",
    "\n",
    "        # ---------------------------------------\n",
    "        # Compute returns using the discriminator\n",
    "        # ---------------------------------------\n",
    "\n",
    "        # TODO...\n",
    "\n",
    "        all_returns.append(returns)\n",
    "\n",
    "    # ======================================\n",
    "    # STEP 2: Prepare data for policy update\n",
    "    # ======================================\n",
    "\n",
    "    # Concatenate all trajectory data into single tensors\n",
    "    all_observations = torch.cat(all_observations, dim=0)  # all_observations.shape = torch.Size([5000, 8])\n",
    "    all_actions = torch.cat(all_actions, dim=0)            # all_actions.shape = torch.Size([5000, 4])\n",
    "    all_returns = torch.cat(all_returns, dim=0)            # all_returns.shape = torch.Size([5000])\n",
    "\n",
    "    # Normalize returns to reduce variance and stabilize training\n",
    "    # This is a common technique in policy gradient methods\n",
    "    all_returns = (all_returns - all_returns.mean()) / (all_returns.std() + 1e-8)\n",
    "\n",
    "    # =================================\n",
    "    # STEP 3: Update the policy network\n",
    "    # =================================\n",
    "\n",
    "    # Compute log probabilities of the actions taken\n",
    "    # The policy outputs action probabilities; we take the log for numerical stability\n",
    "    log_prob_actions_tensor = torch.log(policy(all_observations) + 1e-8)\n",
    "\n",
    "    # Extract log probabilities of the specific actions that were taken\n",
    "    # This multiplies by one-hot encoded actions and sums to get the log prob of the taken action\n",
    "    log_prob_actions_tensor = # TODO...\n",
    "\n",
    "    # Compute policy gradient loss: -E[G_t * log π(a_t|s_t)]\n",
    "    # We maximize expected returns, which is equivalent to minimizing the negative\n",
    "    policy_loss = # TODO...\n",
    "\n",
    "    # Perform gradient descent on the policy\n",
    "    policy_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    policy_optimizer.step()\n",
    "\n",
    "    # =============================================\n",
    "    # STEP 4: Prepare data for discriminator update\n",
    "    # =============================================\n",
    "\n",
    "    # Detach agent actions from the computation graph to avoid coupling with policy gradients\n",
    "    agent_actions_onehot = all_actions.detach().float()\n",
    "\n",
    "    # Sample a batch of expert demonstrations\n",
    "    expert_batch = next(iter(expert_loader))\n",
    "    expert_observations, expert_actions = expert_batch\n",
    "    \n",
    "    # Convert expert actions to one-hot encoding (same format as agent actions)\n",
    "    expert_actions_onehot = torch.nn.functional.one_hot(expert_actions, num_classes=act_dim).float()\n",
    "\n",
    "    # Combine agent and expert data for discriminator training\n",
    "    # Agent observations and actions (detached from policy gradient)\n",
    "    discriminator_input_observations = torch.cat([all_observations.detach(), expert_observations], dim=0)\n",
    "    discriminator_input_actions = torch.cat([agent_actions_onehot, expert_actions_onehot], dim=0)\n",
    "    \n",
    "    # Create binary labels: 0 for agent (fake), 1 for expert (real)\n",
    "    # This follows the standard GAN convention\n",
    "    discriminator_labels = torch.cat([\n",
    "        torch.zeros(len(all_observations)),  # Agent data labeled as 0 (fake)\n",
    "        torch.ones(len(expert_observations))  # Expert data labeled as 1 (real)\n",
    "    ], dim=0)\n",
    "\n",
    "    # Shuffle the combined dataset to prevent the discriminator from learning\n",
    "    # patterns based on the order of agent vs. expert data\n",
    "    perm = torch.randperm(discriminator_input_observations.size(0))\n",
    "    discriminator_input_observations = discriminator_input_observations[perm]\n",
    "    discriminator_input_actions = discriminator_input_actions[perm]\n",
    "    discriminator_labels = discriminator_labels[perm]\n",
    "\n",
    "    # ========================================\n",
    "    # STEP 5: Update the discriminator network\n",
    "    # ========================================\n",
    "\n",
    "    # Train the discriminator to distinguish between agent and expert trajectories\n",
    "    # This is a binary classification task\n",
    "    predictions = # TODO...\n",
    "    discriminator_loss = # TODO...\n",
    "\n",
    "    # Perform gradient descent on the discriminator\n",
    "    discriminator_optimizer.zero_grad()\n",
    "    discriminator_loss.backward()\n",
    "    discriminator_optimizer.step()\n",
    "\n",
    "    # Track metrics\n",
    "    training_history['iteration'].append(iteration)\n",
    "    training_history['discriminator_loss'].append(discriminator_loss.item())\n",
    "    training_history['policy_loss'].append(policy_loss.item())\n",
    "\n",
    "    if iteration % 10 == 0:\n",
    "        print(f\"Iteration {iteration}, Discriminator loss: {discriminator_loss.item()}, Policy loss: {policy_loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(18, 10))\n",
    "\n",
    "# Discriminator Loss\n",
    "axes[0].plot(training_history['iteration'], training_history['discriminator_loss'])\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[0].set_ylabel('Discriminator Loss')\n",
    "axes[0].set_title('Discriminator Loss over Training')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Policy Loss\n",
    "axes[1].plot(training_history['iteration'], training_history['policy_loss'])\n",
    "axes[1].set_xlabel('Iteration')\n",
    "axes[1].set_ylabel('Policy Loss')\n",
    "axes[1].set_title('Policy Loss over Training')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR / \"gail_training_metrics.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the learned policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_DIRNAME = \"lab4_lunar-lander_trained_policy\"\n",
    "\n",
    "env = gym.make('LunarLander-v3', render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(env, FIGS_DIR / VIDEO_DIRNAME)\n",
    "\n",
    "observation, info = env.reset()\n",
    "done = False\n",
    "\n",
    "for t in range(200):\n",
    "    observation_tensor = torch.tensor(observation, dtype=torch.float32).to(device)\n",
    "    action = policy(observation_tensor).argmax().item()\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "env.close()\n",
    "\n",
    "Video(FIGS_DIR / VIDEO_DIRNAME / \"rl-video-episode-0.mp4\", embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "This implementation of GAIL is intentionally kept simple to focus on understanding the core algorithm without the complexity of advanced policy training procedures. We use the **REINFORCE** algorithm (vanilla policy gradient) to train the policy, which makes the learning process transparent and easier to follow.\n",
    "\n",
    "However, this simplicity comes at a cost: the training can be **less efficient and less stable** compared to more sophisticated approaches. In the original GAIL paper ([Ho & Ermon, 2016](https://proceedings.neurips.cc/paper_files/paper/2016/file/cc7e2b878868cbae992d1fb743995d8f-Paper.pdf)), the authors used **Trust Region Policy Optimization (TRPO)** to train the policy, which provides better stability and sample efficiency through constrained optimization.\n",
    "\n",
    "In modern practice, **Proximal Policy Optimization (PPO)** has become the most widely used algorithm for training the policy in GAIL implementations. PPO offers a good balance between:\n",
    "- Implementation simplicity (compared to TRPO)\n",
    "- Training stability (compared to REINFORCE)\n",
    "- Sample efficiency\n",
    "- Robustness across different environments\n",
    "\n",
    "For production-level implementations or more challenging environments, consider replacing the REINFORCE-based policy training with PPO or TRPO to achieve significantly better performance and stability."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
