{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Deep Value-based Reinforcement Learning\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-53439-ep-2025-students/refs/heads/main/assets/logo.jpg?raw=true\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "[CSC_53439_EP-2025](https://moodle.ip-paris.fr/course/view.php?id=10716) Lab session #1\n",
    "\n",
    "2019-2025 Jérémie Decock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jeremiedecock/polytechnique-csc-53439-ep-2025-students/blob/main/lab1_deep_value-based_reinforcement_learning.ipynb)\n",
    "\n",
    "[![My Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jeremiedecock/polytechnique-csc-53439-ep-2025-students/main?filepath=lab1_deep_value-based_reinforcement_learning.ipynb)\n",
    "\n",
    "[![NbViewer](https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg)](https://nbviewer.jupyter.org/github/jeremiedecock/polytechnique-csc-53439-ep-2025-students/blob/main/lab1_deep_value-based_reinforcement_learning.ipynb)\n",
    "\n",
    "[![Local](https://img.shields.io/badge/Local-Save%20As...-blue)](https://github.com/jeremiedecock/polytechnique-csc-53439-ep-2025-students/raw/main/lab1_deep_value-based_reinforcement_learning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The aim of this lab is to provide an in-depth exploration of the most renowned value-based reinforcement learning techniques, specifically *Deep Q-Networks* and its enhancements.\n",
    "\n",
    "In this Python notebook, you will implement and evaluate *Deep Q-Networks* (DQN) and its various adaptations.\n",
    "\n",
    "You can either:\n",
    "- open, edit and execute the notebook in *Google Colab* following this link: https://colab.research.google.com/github/jeremiedecock/polytechnique-csc-53439-ep-2025-students/blob/main/lab1_deep_value-based_reinforcement_learning.ipynb ; this is the **recommended** choice as you have nothing to install on your computer\n",
    "- open, edit and execute the notebook in *MyBinder* (if for any reason the Google Colab solution doesn't work): https://mybinder.org/v2/gh/jeremiedecock/polytechnique-csc-53439-ep-2025-students/main?filepath=lab1_deep_value-based_reinforcement_learning.ipynb\n",
    "- download, edit and execute the notebook on your computer if Python3 and JypyterLab are already installed: https://github.com/jeremiedecock/polytechnique-csc-53439-ep-2025-students/raw/main/lab1_deep_value-based_reinforcement_learning.ipynb\n",
    "\n",
    "If you work with Google Colab or MyBinder, **remember to save or download your work regularly or you may lose it!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please answer the following survey to help us improve this lab session: https://moodle.ip-paris.fr/mod/questionnaire/view.php?id=185677"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Submission\n",
    "\n",
    "Please submit your completed notebook in [Moodle : \"Lab 1 - Submission\"](https://moodle.ip-paris.fr/mod/assign/view.php?id=185679).\n",
    "\n",
    "### Submission Guidelines\n",
    "\n",
    "1. **File Naming:** Rename your notebook as follows: **`firstname_lastname-01.ipynb`** where `firstname` and `lastname` match your email address. *Example: `jesse_read-01.ipynb`*\n",
    "2. **Clear Output Cells:** To reduce file size (**must be under 500 KB**), clear all output cells before submitting. This includes rendered images, videos, plots, and dataframes...\n",
    "   - **JupyterLab:**\n",
    "     - Click **\"Kernel\" → \"Restart Kernel and Clear Outputs of All Cells...\"**\n",
    "     - Then go to **\"File\" → \"Save Notebook As...\"**\n",
    "   - **Google Colab:**\n",
    "     - Click **\"Edit\" → \"Clear all outputs\"**\n",
    "     - Then go to **\"File\" → \"Download\" → \"Download.ipynb\"**\n",
    "   - **VSCode:**\n",
    "     - Click **\"Clear All Outputs\"**\n",
    "     - Then **save your file**\n",
    "3. **Upload Your File:** Only **`.ipynb`** files are accepted.\n",
    "\n",
    "**Note:** Bonus parts (if any) are optional, as their name suggests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Deep value-based reinforcement learning\n",
    "\n",
    "Deep reinforcement learning methods like DQN (Deep Q-Networks) are significant advancements over tabular methods such as Q-Learning because they can handle complex, high-dimensional environments that were previously intractable. While Q-Learning is limited to environments where the state and action spaces are sufficiently small to maintain a table of values, DQN uses neural networks to approximate the Q-value function, allowing it to generalize across similar states and scale to problems with vast state spaces. This enables DQN to learn optimal policies for tasks like video games, robotic control, and other applications where the number of possible states is extraordinarily large.\n",
    "\n",
    "While DQN was designed to tackle large environments like Atari games, the primary focus of this lab is to delve into the underlying algorithms, understand them thoroughly, and evaluate them comprehensively. It's important to note that working with not-so-deep networks captures the essence of deep reinforcement learning, excluding the computational expense. The transition from tabular Q-learning to DQN involves significant implications, primarily due to the ability of DQN to handle high-dimensional state spaces. Moving from DQN to very-deep-DQN is primarily a matter of scale and computational resources. The core principles remain the same, and understanding these principles is the key to mastering reinforcement learning, regardless of the complexity of the network used.\n",
    "For these reasons, in this lab, we will focus on studying the CartPole environment. The CartPole problem is a classic in reinforcement learning, and it provides a simpler and more manageable context for understanding the principles of DQN. The convergence in the CartPole environment is much faster than in Atari games - typically within a minute, as opposed to approximately hours on a well-equipped personal computer for Atari games. This allows us to experiment and iterate more quickly, facilitating a deeper understanding of the algorithms at play.\n",
    "\n",
    "We will therefore focus on algorithms in a simple environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Python environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook relies on several libraries including `gymnasium[classic-control]`, `ipywidgets`, `matplotlib`, `numpy`, `pandas`, `pygame`, `seaborn`, `torch`, and `tqdm`.\n",
    "A complete list of dependencies can be found in the following [requirements_lab1.txt](https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-53439-ep-2025-students/main/requirements_lab1.txt) file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### If you use Google Colab\n",
    "\n",
    "If you use Google Colab, execute the next cell to install required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xt66Z85AmOI2",
    "outputId": "bd7a6b75-ad3c-4be1-d560-8239fbc0e9d2"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def is_colab():\n",
    "    return \"google.colab\" in sys.modules\n",
    "\n",
    "def run_subprocess_command(cmd):\n",
    "    # run the command\n",
    "    process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n",
    "    # print the output\n",
    "    for line in process.stdout:\n",
    "        print(line.decode().strip())\n",
    "\n",
    "if is_colab():\n",
    "    # run_subprocess_command(\"apt install xvfb x11-utils\")\n",
    "    run_subprocess_command(\"pip install -r https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-53439-ep-2025-students/main/requirements_lab1_google_colab.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you have downloaded the notebook on your computer and execute it in your own Python environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up the necessary dependencies, run the following commands to establish a [Python virtual environment (venv)](https://docs.python.org/3/library/venv.html) that includes all the essential libraries for this lab.\n",
    "\n",
    "#### On Posix systems (Linux, MacOSX, WSL, ...)\n",
    "\n",
    "```bash\n",
    "python3 -m venv env-lab1\n",
    "source env-lab1/bin/activate\n",
    "python3 -m pip install --upgrade pip\n",
    "python3 -m pip install -r https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-53439-ep-2025-students/main/requirements_lab1.txt\n",
    "```\n",
    "\n",
    "#### On Windows\n",
    "\n",
    "```bash\n",
    "python3 -m venv env-lab1\n",
    "env-lab1\\Scripts\\activate.bat\n",
    "python3 -m pip install --upgrade pip\n",
    "python3 -m pip install -r https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-53439-ep-2025-students/main/requirements_lab1.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run CSC-53439-EP notebooks locally in a dedicated Docker container\n",
    "\n",
    "If you are familiar with Docker, an image is available on Docker Hub for this lab:\n",
    "\n",
    "```bash\n",
    "docker run -it --rm --user root -p 8888:8888 -e NB_UID=$(id -u) -e NB_GID=$(id -g) -v \"${PWD}\":/home/jovyan/work jdhp/csc-53439-ep-lab1:latest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import gymnasium as gym\n",
    "import itertools\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from typing import List, Tuple, Deque, Optional, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGS_DIR = Path(\"figs/\") / \"lab1\"       # Where to save figures (.gif or .mp4 files)\n",
    "PLOTS_DIR = Path(\"figs/\") / \"lab1\"      # Where to save plots (.png or .svg files)\n",
    "MODELS_DIR = Path(\"models/\") / \"lab1\"   # Where to save models (.pth files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not FIGS_DIR.exists():\n",
    "    FIGS_DIR.mkdir(parents=True)\n",
    "if not PLOTS_DIR.exists():\n",
    "    PLOTS_DIR.mkdir(parents=True)\n",
    "if not MODELS_DIR.exists():\n",
    "    MODELS_DIR.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Gymnasium rendering wrapper to visualize environments as GIF images within the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook allows you to visualize the episodes as animated GIFs. Run the cell below to enable this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To display GIF images in the notebook\n",
    "\n",
    "import imageio     # To render episodes in GIF images (otherwise there would be no render on Google Colab)\n",
    "                   # C.f. https://stable-baselines.readthedocs.io/en/master/guide/examples.html#bonus-make-a-gif-of-a-trained-agent\n",
    "import IPython\n",
    "from IPython.display import Image\n",
    "\n",
    "if is_colab():\n",
    "    import pyvirtualdisplay\n",
    "\n",
    "    _display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
    "                                        size=(1400, 900))\n",
    "    _ = _display.start()\n",
    "\n",
    "class RenderWrapper:\n",
    "    def __init__(self, env, force_gif=False):\n",
    "        self.env = env\n",
    "        self.force_gif = force_gif\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.images = []\n",
    "\n",
    "    def render(self):\n",
    "        if not is_colab():\n",
    "            self.env.render()\n",
    "            time.sleep(1./60.)\n",
    "\n",
    "        if is_colab() or self.force_gif:\n",
    "            img = self.env.render()         # Assumes env.render_mode == 'rgb_array'\n",
    "            self.images.append(img)\n",
    "\n",
    "    def make_gif(self, filename=\"render\"):\n",
    "        if is_colab() or self.force_gif:\n",
    "            gif_path = filename.with_suffix('.gif')\n",
    "            imageio.mimsave(gif_path, [np.array(img) for i, img in enumerate(self.images) if i%2 == 0], fps=29, loop=0)\n",
    "            return Image(open(gif_path,'rb').read())\n",
    "\n",
    "    @classmethod\n",
    "    def register(cls, env, force_gif=False):\n",
    "        env.render_wrapper = cls(env, force_gif=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of trainings\n",
    "\n",
    "To achieve more representative outcomes at the conclusion of each exercise, we average the results across multiple training sessions. The `DEFAULT_NUMBER_OF_TRAININGS` variable specifies the number of training sessions conducted before the results are displayed. \n",
    "\n",
    "We recommend setting a lower value (such as 1 or 2) during the development and testing phases of your implementations. Once you have completed your work and are confident in its functionality, you can increase the number of training sessions to minimize the variance in results. Be aware that a higher number of training sessions will extend the execution time, so adjust this setting in accordance with your computer's capabilities.\n",
    "\n",
    "Additionally, you have the option to assign a specific value to the `NUMBER_OF_TRAININGS` variable for each exercise directly within the cells where the training loop is defined (the `NUMBER_OF_TRAININGS` variable is commented out at the beginning of these cells)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_NUMBER_OF_TRAININGS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## PyTorch Refresher and Cheat Sheet\n",
    "\n",
    "In this lab, we will be implementing our deep reinforcement learning algorithms using PyTorch.\n",
    "If you need a refresher, you might find this [PyTorch Cheat Sheet](https://web.archive.org/web/20241215022731/https://pytorch.org/tutorials/beginner/ptcheat.html) helpful. It provides a quick reference for many of the most commonly used PyTorch functions and concepts, and can be a valuable resource as you work through this lab.\n",
    "\n",
    "You can also refer to the [official documentation](https://pytorch.org/docs/stable/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch can run on both CPUs and GPUs. The following cell will determine the device PyTorch will use. If a GPU is available, PyTorch will use it; otherwise, it will use the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Set the device to CUDA if available, otherwise use CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For utilizing a GPU on Google Colab, you also have to activate it following the steps outlined [here](https://colab.research.google.com/notebooks/gpu.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Available GPUs:\")\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"- Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"- No GPU available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a very recent GPU and want to use it, you might need to install a specific version of PyTorch compatible with your Cuda version. For this, you will have to edit the [requirements-lab1.txt](https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-53439-ep-2025-students/main/requirements_lab1.txt) file and replace the current version of PyTorch with the one compatible with your Cuda version. Check the [official PyTorch website](https://pytorch.org/get-started/locally/) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the GPU is not very useful in this lab because CartPole is a simple and quick problem to solve, and CUDA spends more time transferring data between the CPU and GPU than processing it directly on the CPU.\n",
    "\n",
    "You can uncomment the next cell to explicitly instruct PyTorch to train neural networks using the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PyTorch will train and test neural networks on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus section 1: Monitoring the training process with Tensorboard and Aim\n",
    "\n",
    "Monitoring the training process is crucial for understanding the agent's learning progress. In this section, we will introduce two open-source tools that can help you visualize the training process: Tensorboard and Aim.\n",
    "\n",
    "Tensorflow and Aim are particularly useful for monitoring the training process on a local machine, as they do not require a server to run.\n",
    "Other solutions will be covered in the next tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring the training process with Tensorboard\n",
    "\n",
    "TensorBoard is a visualization tool developed by Google, primarily used to track and visualize metrics during the training of deep learning models. While initially designed for TensorFlow, it can also be integrated with **PyTorch** through the **torch.utils.tensorboard** API. This allows real-time tracking of metrics such as losses, accuracy, computation graphs, and even examining images, weight distributions, histograms, and more.\n",
    "\n",
    "The following cell demonstrates how to use TensorBoard with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Simple example: model and data\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Creating a model, optimizer, and loss function\n",
    "model = SimpleModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Initializing TensorBoard SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Training example\n",
    "for epoch in range(100):\n",
    "    # Fake data\n",
    "    inputs = torch.randn(32, 10)\n",
    "    targets = torch.randn(32, 1)\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_fn(outputs, targets)\n",
    "    \n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Log the loss values to TensorBoard\n",
    "    writer.add_scalar('Loss/train', loss.item(), epoch)\n",
    "\n",
    "# Closing SummaryWriter\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`SummaryWriter`** is the central tool for logging data to TensorBoard. It captures metric values, like loss, for each iteration/epoch.\n",
    "\n",
    "This example trains a simple model on fake data (32 samples with 10 features) over 100 epochs, and at each epoch, the loss value is sent to TensorBoard via `writer.add_scalar`.\n",
    "\n",
    "To visualize the data from a local machine, run the following command in the terminal:\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "then open a web browser and go at the address mentioned in the terminal (usually http://localhost:6006/).\n",
    "\n",
    "If you are using Google Colab, you can use the **TensorBoard magic** to visualize the data directly in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard extension in Colab\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For mor information, check the Official Documentation:\n",
    "- [TensorBoard for PyTorch Documentation](https://pytorch.org/docs/stable/tensorboard.html)\n",
    "- [Official TensorBoard Documentation (General)](https://www.tensorflow.org/tensorboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring the training process with AIM\n",
    "\n",
    "Aim is an open-source alternative to TensorBoard for managing and visualizing machine learning experiments.\n",
    "Designed to be simpler and more flexible, **Aim** allows tracking and analyzing metrics, hyperparameters, model outputs, and more for projects using PyTorch, TensorFlow, or other frameworks.\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/13848158/136374529-af267918-5dc6-4a4e-8ed2-f6333a332f96.gif\" width=\"600px\"></img>\n",
    "\n",
    "The following cell demonstrates how to use Aim with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import aim\n",
    "\n",
    "# Initialize Aim\n",
    "run = aim.Run()\n",
    "\n",
    "# set training hyperparameters\n",
    "run['hparams'] = {\n",
    "    'learning_rate': 0.01,\n",
    "    'batch_size': 32,\n",
    "}\n",
    "\n",
    "# Simple model example\n",
    "model = torch.nn.Linear(1, 1)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(10):\n",
    "    inputs = torch.randn(10, 1)\n",
    "    targets = torch.randn(10, 1)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Track the 'loss' metric\n",
    "    run.track(loss.item(), name='loss', epoch=epoch)    # Log metric with Aim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example trains a simple model on fake data over 10 epochs, and at each epoch, the loss value is sent to Aim via `run.track`.\n",
    "\n",
    "To visualize the data from a local machine, run the following command in the terminal:\n",
    "\n",
    "```bash\n",
    "aim up\n",
    "```\n",
    "\n",
    "then open a web browser and go at the address mentioned in the terminal (usually http://localhost:43800/).\n",
    "\n",
    "Or alternatively, you can use the **Aim magic** to [visualize the data directly in the notebook](https://aimstack.readthedocs.io/en/latest/using/jupyter_notebook_ui.html) (for instance if you are using Google Colab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext aim\n",
    "%aim up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information, check the official Documentation:\n",
    "- [Aim Documentation](https://aimstack.readthedocs.io/en/latest/)\n",
    "- [GitHub Aim](https://github.com/aimhubio/aim)\n",
    "- [Quick start](https://github.com/aimhubio/aim#-quick-start)\n",
    "- [Getting started](https://aimstack.readthedocs.io/en/latest/quick_start/setup.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Hands on Cart Pole environment [reminder]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of focusing on the algorithms, we will use standard environments provided by the Gymnasium suite.\n",
    "Gymnasium provides controllable environments (https://gymnasium.farama.org/environments/classic_control/) for research in reinforcement learning.\n",
    "Especially, we will try to solve the CartPole-v1 environment (c.f. https://gymnasium.farama.org/environments/classic_control/cart_pole/) which offers a continuous state space and discrete action space.\n",
    "The Cart Pole task consists in maintaining a pole in a vertical position by moving a cart on which the pole is attached with a joint.\n",
    "No friction is considered.\n",
    "The task is supposed to be solved if the pole stays up-right (within 15 degrees) for 200 steps in average over 100 episodes while keeping the cart position within reasonable bounds.\n",
    "The state is given by $\\{x,\\frac{d x}{d t},\\omega,\\frac{d \\omega}{d t}\\}$ where $x$ is the position of the cart and $\\omega$ is the angle between the pole and vertical position.\n",
    "There are only two possible actions: $a \\in \\{0, 1\\}$ where $a = 0$ means \"push the cart to the LEFT\" and $a = 1$ means \"push the cart to the RIGHT\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Hands on Cart Pole\n",
    "\n",
    "**Task 1:** refer to the following link [CartPole Environment](https://gymnasium.farama.org/environments/classic_control/cart_pole/) to familiarize yourself with the CartPole environment if you are not already.\n",
    "\n",
    "**Note:** for a refresher on the key concepts of Gymnasium, you can visit this [Basic Usage Guide](https://gymnasium.farama.org/introduction/basic_usage/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some information about the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n.item()\n",
    "\n",
    "print(f\"State space size is: { env.observation_space }\")\n",
    "print(f\"Action space size is: { env.action_space }\")\n",
    "print(\"Actions are: {\" + \", \".join([str(a) for a in range(env.action_space.n)]) + \"}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:** Run the following cells and check different basic \n",
    "policies (for instance constant actions or randomly drawn actions) to discover the CartPole environment.\n",
    "Although this environment has easy dynamics that can be computed analytically, we will solve this problem with Policy Gradient based Reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test the CartPole environment with a constant policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "observation, info = env.reset()\n",
    "done = False\n",
    "\n",
    "for t in range(50):\n",
    "    env.render_wrapper.render()\n",
    "\n",
    "    if not done:\n",
    "        print(observation)\n",
    "    else:\n",
    "        print(\"x\", end=\"\")\n",
    "\n",
    "    action = 0\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "\n",
    "print()\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(FIGS_DIR / \"lab1_ex1left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "observation, info = env.reset()\n",
    "done = False\n",
    "\n",
    "for t in range(50):\n",
    "    env.render_wrapper.render()\n",
    "\n",
    "    if not done:\n",
    "        print(observation)\n",
    "    else:\n",
    "        print(\"x\", end=\"\")\n",
    "\n",
    "    action = 1\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "print()\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(FIGS_DIR / \"lab1_ex1right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the CartPole environment with a random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "for episode_index in range(5):\n",
    "    observation, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    for t in range(70):\n",
    "        env.render_wrapper.render()\n",
    "\n",
    "        if not done:\n",
    "            print(observation)\n",
    "        else:\n",
    "            print(\"x\", end=\"\")\n",
    "\n",
    "\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "    print()\n",
    "    env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(FIGS_DIR / \"lab1_ex1random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Deep value-based reinforcement learning with Deep Q-Networks (DQN) [reminder]\n",
    "\n",
    "In this part, we will begin our exploration of deep reinforcement learning with Deep Q-Networks (DQN), a famous value-based method.\n",
    "\n",
    "Deep reinforcement learning methods like DQN (Deep Q-Networks) are significant advancements over tabular methods such as Q-Learning because they can handle complex, high-dimensional environments that were previously intractable. While Q-Learning is limited to environments where the state and action spaces are sufficiently small to maintain a table of values, DQN uses neural networks to approximate the Q-value function, allowing it to generalize across similar states and scale to problems with vast state spaces. This enables DQN to learn optimal policies for tasks like video games, robotic control, and other applications where the number of possible states is extraordinarily large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement a naive value-based deep reinforcement learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step will be to write a naive implementation of a version of Q-Learning, where the Q-function is approximated by a neural network. This approach combines traditional Q-Learning with the power of function approximation provided by neural networks, allowing us to handle environments with large state spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Input</b>:<br>\n",
    "\t$\\quad\\quad$ none<br>\n",
    "<b>Algorithm parameter</b>:<br>\n",
    "\t$\\quad\\quad$ discount factor $\\gamma$<br>\n",
    "\t$\\quad\\quad$ step size $\\alpha \\in (0,1]$<br>\n",
    "\t$\\quad\\quad$ small $\\epsilon > 0$<br><br>\n",
    "\n",
    "<b>FOR EACH</b> episode<br>\n",
    "\t$\\quad$ $\\mathbf{s} \\leftarrow \\text{env.reset}()$<br>\n",
    "\t$\\quad$ <b>DO</b> <br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{a} \\leftarrow \\epsilon\\text{-greedy}(\\mathbf{s}, \\hat{Q}_{\\mathbf{\\omega}}$ $)$<br>\n",
    "\t\t$\\quad\\quad$ $r, \\mathbf{s'} \\leftarrow \\text{env.step}(\\mathbf{a})$<br>\n",
    "\t\t$\\quad\\quad$ $y \\leftarrow\n",
    "\t\t\\begin{cases}\n",
    "\t\t\tr & \\text{for terminal } \\mathbf{s'}\\\\\n",
    "\t\t\tr + \\gamma \\max_{\\mathbf{a}^\\star \\in \\mathcal{A}} \\hat{Q}_{\\mathbf{\\omega}}(\\mathbf{s'})_{\\mathbf{a}^\\star} & \\text{for non-terminal } \\mathbf{s'}\n",
    "\t\t\\end{cases}$<br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{\\omega} \\leftarrow \\mathbf{\\omega} + \\alpha \\left[ y - \\hat{Q}_{\\mathbf{\\omega}}(\\mathbf{s})_{\\mathbf{a}} \\right] ~ \\nabla_{\\mathbf{\\omega}} \\hat{Q}_{\\mathbf{\\omega}}(\\mathbf{s})_{\\mathbf{a}}$ <br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{s} \\leftarrow \\mathbf{s'}$ <br>\n",
    "\t$\\quad$ <b>UNTIL</b> $\\mathbf{s}$ is final<br><br>\n",
    "<b>RETURN</b> $\\mathbf{\\omega}$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the Q-network\n",
    "\n",
    "The Q-Network is used to approximate the action value function, which gives the expected future reward for taking a particular action in a particular state. The network is trained to minimize the difference between its predicted Q-values and the actual return received.\n",
    "\n",
    "In this exercise, the Q-Network is implemented as a simple feedforward neural network with two hidden layers. The input to the network consists of the observed state, and the output provides a Q-value for each possible action.  \n",
    "Each hidden layer applies the ReLU activation function, while the output layer uses a linear activation function.\n",
    "\n",
    "**Important note:** The \"naive\" agent you will implement in this exercise is **not** a DQN agent and **will not be able to solve the CartPole environment**. However, it serves as a stepping stone toward implementing the DQN agent in Exercises 3 and 4.\n",
    "\n",
    "**Task 2.1:** implement the constructor and the `forward` method of the Q-network we will use in our RL agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A Q-Network implemented with PyTorch.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    layer1 : torch.nn.Linear\n",
    "        First fully connected layer.\n",
    "    layer2 : torch.nn.Linear\n",
    "        Second fully connected layer.\n",
    "    layer3 : torch.nn.Linear\n",
    "        Third fully connected layer.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(x: torch.Tensor) -> torch.Tensor\n",
    "        Define the forward pass of the QNetwork.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_observations: int, n_actions: int, nn_l1: int, nn_l2: int):\n",
    "        \"\"\"\n",
    "        Initialize a new instance of QNetwork.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_observations : int\n",
    "            The size of the observation space.\n",
    "        n_actions : int\n",
    "            The size of the action space.\n",
    "        nn_l1 : int\n",
    "            The number of neurons on the first layer.\n",
    "        nn_l2 : int\n",
    "            The number of neurons on the second layer.\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        # TODO...\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Define the forward pass of the QNetwork.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            The input tensor (state).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The output tensor (Q-values).\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO...\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement an inference function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parctical tips\n",
    "\n",
    "To complete task 2.2, you need to be familiar with the following PyTorch concepts:\n",
    "\n",
    "- [torch.squeeze](https://pytorch.org/docs/stable/generated/torch.squeeze.html) (or [torch.Tensor.squeeze](https://pytorch.org/docs/stable/generated/torch.Tensor.squeeze.html)) / [torch.unsqueeze](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html) (or [torch.Tensor.unsqueeze](https://pytorch.org/docs/stable/generated/torch.Tensor.unsqueeze.html)) to add / remove a batch dimension.\n",
    "- [torch.argmax](https://pytorch.org/docs/stable/generated/torch.argmax.html) (or [torch.Tensor.argmax](https://pytorch.org/docs/stable/generated/torch.Tensor.argmax.html)) to get the index of the maximum value of a tensor.\n",
    "- [torch.Tensor.item](https://pytorch.org/docs/stable/generated/torch.Tensor.item.html) to get the value of a tensor with a single element as a standard Python number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.2:** Your next assignment is to complete the function below, which will be used to evaluate the performance of an agent in a simulated environment over one or multiple episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_q_network_agent(env: gym.Env, q_network: torch.nn.Module, num_episode: int = 1, render: bool = True) -> List[int]:\n",
    "    \"\"\"\n",
    "    Test a naive agent in the given environment using the provided Q-network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment in which to test the agent.\n",
    "    q_network : torch.nn.Module\n",
    "        The Q-network to use for decision making.\n",
    "    num_episode : int, optional\n",
    "        The number of episodes to run, by default 1.\n",
    "    render : bool, optional\n",
    "        Whether to render the environment, by default True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[int]\n",
    "        A list of rewards per episode.\n",
    "    \"\"\"\n",
    "    episode_reward_list = []\n",
    "\n",
    "    for episode_id in range(num_episode):\n",
    "\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render_wrapper.render()\n",
    "\n",
    "            # Convert the state to a PyTorch tensor and add a batch dimension (unsqueeze)\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "            # q_values = TODO... # Compute the Q-values for the current state using the Q-network\n",
    "\n",
    "            # action = TODO... # Select the action with the highest Q-value\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        episode_reward_list.append(episode_reward)\n",
    "        print(f\"Episode reward: {episode_reward}\")\n",
    "\n",
    "    return episode_reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.3:** Test this function on the untrained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_network = QNetwork(state_dim, action_dim, nn_l1=128, nn_l2=128).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "test_q_network_agent(env, q_network, num_episode=5)\n",
    "\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(FIGS_DIR / \"lab1_dqn_naive_untained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the epsilon greedy function\n",
    "\n",
    "In the following cell, you will implement the epsilon-greedy strategy, which is a crucial component in balancing exploration and exploitation during the learning process of our reinforcement learning agent.\n",
    "In this implementation, the epsilon value will decrease over time to encourage the agent to explore more in the early stages of training and exploit more as training progresses. After a certain number of episodes, the epsilon value will remain constant at a minimum value.\n",
    "\n",
    "The figure below illustrates the usage of `epsilon`, `epsilon_min` and `epsilon_start`:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-53439-ep-2025-students/main/assets/lab1_epsilon.svg\" width=\"600px\" />\n",
    "\n",
    "**Task 2.4:** Now, let's proceed to implement the epsilon-greedy strategy, which is a crucial component in balancing exploration and exploitation during the learning process of our reinforcement learning agent. To accomplish this, complete the `__call__` function in the following code block.\n",
    "\n",
    "**Tips:** `EpsilonGreedy` is a *[callable](https://docs.python.org/3/glossary.html#term-callable) class* (a.k.a a Python *functor*) that can be used as a function. The `__call__` method is called when the instance is \"called\" as a function. In this case, the `__call__` method should return a random action with probability `epsilon` and the action with the highest Q-value with probability `1 - epsilon`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy:\n",
    "    \"\"\"\n",
    "    An Epsilon-Greedy policy.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    epsilon : float\n",
    "        The initial probability of choosing a random action.\n",
    "    epsilon_min : float\n",
    "        The minimum probability of choosing a random action.\n",
    "    epsilon_decay : float\n",
    "        The decay rate for the epsilon value after each action.\n",
    "    env : gym.Env\n",
    "        The environment in which the agent is acting.\n",
    "    q_network : torch.nn.Module\n",
    "        The Q-Network used to estimate action values.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __call__(state: np.ndarray) -> np.int64\n",
    "        Select an action for the given state using the epsilon-greedy policy.\n",
    "    decay_epsilon()\n",
    "        Decay the epsilon value after each action.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 epsilon_start: float,\n",
    "                 epsilon_min: float,\n",
    "                 epsilon_decay:float,\n",
    "                 env: gym.Env,\n",
    "                 q_network: torch.nn.Module):\n",
    "        \"\"\"\n",
    "        Initialize a new instance of EpsilonGreedy.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        epsilon_start : float\n",
    "            The initial probability of choosing a random action.\n",
    "        epsilon_min : float\n",
    "            The minimum probability of choosing a random action.\n",
    "        epsilon_decay : float\n",
    "            The decay rate for the epsilon value after each episode.\n",
    "        env : gym.Env\n",
    "            The environment in which the agent is acting.\n",
    "        q_network : torch.nn.Module\n",
    "            The Q-Network used to estimate action values.\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.env = env\n",
    "        self.q_network = q_network\n",
    "\n",
    "    def __call__(self, state: np.ndarray) -> np.int64:\n",
    "        \"\"\"\n",
    "        Select an action for the given state using the epsilon-greedy policy.\n",
    "\n",
    "        If a randomly chosen number is less than epsilon, a random action is chosen.\n",
    "        Otherwise, the action with the highest estimated action value is chosen.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state : np.ndarray\n",
    "            The current state of the environment.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.int64\n",
    "            The chosen action.\n",
    "        \"\"\"\n",
    "\n",
    "        if random.random() < self.epsilon:\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                # state_tensor = TODO... # Convert the state to a PyTorch tensor and add a batch dimension (unsqueeze)\n",
    "\n",
    "                # q_values = TODO... # Compute the Q-values for the current state using the Q-network\n",
    "\n",
    "                # action = TODO... # Select the action with the highest Q-value\n",
    "\n",
    "        return action\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"\n",
    "        Decay the epsilon value after each episode.\n",
    "\n",
    "        The new epsilon value is the maximum of `epsilon_min` and the product of the current \n",
    "        epsilon value and `epsilon_decay`.\n",
    "        \"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing a Learning Rate Scheduler\n",
    "\n",
    "The following cell introduces a PyTorch Learning Rate (LR) scheduler. This scheduler is used for managing and adjusting the learning rate ($\\alpha$ in the \"naive value-based reinforcement learning algorithm\") throughout the training process of our agent. It is designed to adjust the learning rate of an optimizer at each *epoch*, following an exponential decay strategy, but with a lower limit on the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimumExponentialLR(torch.optim.lr_scheduler.ExponentialLR):\n",
    "    def __init__(self, optimizer: torch.optim.Optimizer, lr_decay: float, last_epoch: int = -1, min_lr: float = 1e-6):\n",
    "        \"\"\"\n",
    "        Initialize a new instance of MinimumExponentialLR.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        optimizer : torch.optim.Optimizer\n",
    "            The optimizer whose learning rate should be scheduled.\n",
    "        lr_decay : float\n",
    "            The multiplicative factor of learning rate decay.\n",
    "        last_epoch : int, optional\n",
    "            The index of the last epoch. Default is -1.\n",
    "        min_lr : float, optional\n",
    "            The minimum learning rate. Default is 1e-6.\n",
    "        \"\"\"\n",
    "        self.min_lr = min_lr\n",
    "        super().__init__(optimizer, lr_decay, last_epoch=-1)\n",
    "\n",
    "    def get_lr(self) -> List[float]:\n",
    "        \"\"\"\n",
    "        Compute learning rate using chainable form of the scheduler.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[float]\n",
    "            The learning rates of each parameter group.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            max(base_lr * self.gamma ** self.last_epoch, self.min_lr)\n",
    "            for base_lr in self.base_lrs\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing the Training Function\n",
    "\n",
    "The following function is the final component of our initial agent. It orchestrates the training process, enabling the agent to learn from its interactions with the environment.\n",
    "\n",
    "During each episode, the agent selects actions based on an epsilon-greedy policy, observes the next state and reward from the environment, and updates the weights of the Q-Network based on the observed reward and the maximum predicted Q-value of the next state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parctical tips\n",
    "\n",
    "To compute the target value, you can eliminate the need for an if statement to differentiate between terminal and non-terminal states by using the following formula:  \n",
    "\n",
    "$$\n",
    "y = r + \\gamma \\max_{\\mathbf{a}^\\star \\in \\mathcal{A}} \\hat{Q}_{\\mathbf{\\omega}}(\\mathbf{s'})_{\\mathbf{a}^\\star} \\times (1 - \\text{done})\n",
    "$$\n",
    "\n",
    "where $\\text{done} = 1$ if $s'$ is a terminal state and 0 otherwise.\n",
    "\n",
    "To complete task 2.5, you also need to be familiar with the following PyTorch concepts:\n",
    "\n",
    "- [torch.max](https://pytorch.org/docs/stable/generated/torch.max.html) (or [torch.Tensor.max](https://pytorch.org/docs/stable/generated/torch.Tensor.max.html)) to get the maximum value of a tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.5:** complete this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_agent(env: gym.Env,\n",
    "                      q_network: torch.nn.Module,\n",
    "                      optimizer: torch.optim.Optimizer,\n",
    "                      loss_fn: Callable,\n",
    "                      epsilon_greedy: EpsilonGreedy,\n",
    "                      device: torch.device,\n",
    "                      lr_scheduler: _LRScheduler,\n",
    "                      num_episodes: int,\n",
    "                      gamma: float) -> List[float]:\n",
    "    \"\"\"\n",
    "    Train the Q-network on the given environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment to train on.\n",
    "    q_network : torch.nn.Module\n",
    "        The Q-network to train.\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer to use for training.\n",
    "    loss_fn : callable\n",
    "        The loss function to use for training.\n",
    "    epsilon_greedy : EpsilonGreedy\n",
    "        The epsilon-greedy policy to use for action selection.\n",
    "    device : torch.device\n",
    "        The device to use for PyTorch computations.\n",
    "    lr_scheduler : torch.optim.lr_scheduler._LRScheduler\n",
    "        The learning rate scheduler to adjust the learning rate during training.\n",
    "    num_episodes : int\n",
    "        The number of episodes to train for.\n",
    "    gamma : float\n",
    "        The discount factor for future rewards.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[float]\n",
    "        A list of cumulated rewards per episode.\n",
    "    \"\"\"\n",
    "    episode_reward_list = []\n",
    "\n",
    "    for episode_index in tqdm(range(1, num_episodes)):\n",
    "        state, info = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for t in itertools.count():\n",
    "\n",
    "\n",
    "            # Get action, next_state and reward\n",
    "\n",
    "            action = epsilon_greedy(state)\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Update the q_network weights\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # next_state_tensor = TODO... # Convert the next_state to a PyTorch tensor and add a batch dimension (unsqueeze)\n",
    "\n",
    "                # target = TODO... # Compute the **target** Q-value\n",
    "\n",
    "            # state_tensor = TODO... # Convert the state to a PyTorch tensor and add a batch dimension (unsqueeze)\n",
    "\n",
    "            # q_values = TODO... # Compute the Q-values for the current state using the Q-network\n",
    "\n",
    "            # q_value_of_current_action = TODO... # Get the Q-value of the current action\n",
    "\n",
    "            loss = loss_fn(q_value_of_current_action, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "\n",
    "        episode_reward_list.append(episode_reward)\n",
    "        epsilon_greedy.decay_epsilon()\n",
    "\n",
    "    return episode_reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "NUMBER_OF_TRAININGS = DEFAULT_NUMBER_OF_TRAININGS    # Change the default (global) value here if you want a specific number of trainings for this exercise\n",
    "naive_trains_result_list = [[], [], []]\n",
    "\n",
    "for train_index in range(NUMBER_OF_TRAININGS):\n",
    "\n",
    "    # Instantiate required objects\n",
    "\n",
    "    q_network = QNetwork(state_dim, action_dim, nn_l1=128, nn_l2=128).to(device)\n",
    "    optimizer = torch.optim.AdamW(q_network.parameters(), lr=0.004, amsgrad=True)\n",
    "    #lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "    lr_scheduler = MinimumExponentialLR(optimizer, lr_decay=0.97, min_lr=0.0001)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    epsilon_greedy = EpsilonGreedy(epsilon_start=0.82, epsilon_min=0.013, epsilon_decay=0.9675, env=env, q_network=q_network)\n",
    "\n",
    "    # Train the q-network\n",
    "\n",
    "    episode_reward_list = train_naive_agent(env,\n",
    "                                            q_network,\n",
    "                                            optimizer,\n",
    "                                            loss_fn,\n",
    "                                            epsilon_greedy,\n",
    "                                            device,\n",
    "                                            lr_scheduler,\n",
    "                                            num_episodes=150,\n",
    "                                            gamma=0.9)\n",
    "    naive_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
    "    naive_trains_result_list[1].extend(episode_reward_list)\n",
    "    naive_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
    "\n",
    "naive_trains_result_df = pd.DataFrame(np.array(naive_trains_result_list).T, columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"])\n",
    "naive_trains_result_df[\"agent\"] = \"Naive\"\n",
    "\n",
    "# Save the action-value estimation function of the last train\n",
    "\n",
    "torch.save(q_network, MODELS_DIR / \"lab1_naive_q_network.pth\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", estimator=None, units=\"training_index\", data=naive_trains_result_df,\n",
    "                height=7, aspect=2, alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", hue=\"agent\", kind=\"line\", data=naive_trains_result_df, height=7, aspect=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_network = torch.load(\"naive_q_network.pth\").to(device)\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "test_q_network_agent(env, q_network, num_episode=5)\n",
    "\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(FIGS_DIR / \"lab1_dqn_naive_tained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why It Doesn't Work: The Complexity of Deep Reinforcement Learning\n",
    "\n",
    "Our initial deep value-based agent did not converge, primarily due to the three fundamental challenges of value-based deep reinforcement learning:\n",
    "\n",
    "1. **Coverage**: Convergence to the optimal Q-function relies on comprehensive coverage of the state space. However, in the context of deep RL, the state space is often too large to be fully covered. In situations where not all states are sampled due to their vast number, the guarantee of convergence no longer holds.\n",
    "\n",
    "2. **Correlation**: The probability of transitioning to the next state is highly influenced by the current state. This strong correlation can lead to local overfitting and the risk of becoming trapped in a local optimum: the neural network, which approximates the Q-function, may become overly specialized in a small portion of the action-state space and neglect the rest.\n",
    "\n",
    "3. **Convergence**: The \"targets\" used as the truth to be achieved \"move\" during the learning process. For the same prediction (estimation of the value of a state-action pair, i.e., its Q-value), the loss of a given example changes during the learning process (due to *bootstrapping* a main concept of TD-Learning). In other words, DQN tries to minimize a moving target, a target that depends on the model we are learning and optimizing. This can lead to instability and make it difficult for the learning process to converge to an optimal policy.\n",
    "\n",
    "In the following sections, we will explore strategies to address these challenges and improve the performance of our deep reinforcement learning agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Implement Deep Q-Networks v1 (DQN version 2013 with experience replay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2013, DeepMind made a significant contribution to the field of reinforcement learning with the publication of the paper \"Playing Atari with Deep Reinforcement Learning\" by Volodymyr Mnih and al (https://arxiv.org/abs/1312.5602). This paper marked the introduction of the first version of Deep Q-Networks (DQN).\n",
    "\n",
    "The paper's primary innovation was the development of a technique to decorrelate states in reinforcement learning. This technique, known as *experience replay*, leverages a *replay buffer* to store and sample experiences. The introduction of experience replay greatly enhanced the stability and efficiency of the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Experience replay\n",
    "\n",
    "Experience replay is a key technique used in Deep Q-Networks (DQN) to address the issues of correlation.\n",
    "\n",
    "In a typical reinforcement learning setup, an agent learns by interacting with the environment, receiving feedback in the form of rewards, and updating its policy based on this feedback. This process is inherently sequential and the successive states are highly correlated, which can lead to overfitting and instability in learning.\n",
    "\n",
    "Experience replay addresses these issues by storing the agent's experiences, i.e., the tuples of (state, action, reward, next state), in a data structure known as a replay buffer. During the learning process, instead of learning from the most recent experience, the agent randomly samples a batch of experiences from the replay buffer. This random sampling breaks the correlation between successive experiences, leading to more stable and robust learning.\n",
    "\n",
    "Also, by learning from past experiences, the agent can effectively learn from a fixed target, which mitigates the issue of learning from a moving target. This is because the experiences in the replay buffer remain fixed once they are stored, even though the agent's policy continues to evolve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN v2013 Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: main differences with the previous algorithm are highlighted in red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Input</b>:<br>\n",
    "\t$\\quad\\quad$ none<br>\n",
    "<b>Algorithm parameter</b>:<br>\n",
    "\t$\\quad\\quad$ discount factor $\\gamma$<br>\n",
    "\t$\\quad\\quad$ step size $\\alpha \\in (0,1]$<br>\n",
    "\t$\\quad\\quad$ small $\\epsilon > 0$<br>\n",
    "\t$\\quad\\quad$ capacity of the experience replay memory $\\color{red}{M}$<br>\n",
    "\t$\\quad\\quad$ batch size $\\color{red}{m}$<br><br>\n",
    "\n",
    "<b>Initialize</b> replay memory $\\color{red}{\\mathcal{D}}$ to capacity $M$<br>\n",
    "<b>Initialize</b> action-value function $\\hat{Q}$ with random weights $\\mathbf{\\omega}$<br><br>\n",
    "\n",
    "<b>FOR EACH</b> episode<br>\n",
    "\t$\\quad$ $\\mathbf{s} \\leftarrow \\text{env.reset}()$<br>\n",
    "\t$\\quad$ <b>DO</b> <br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{a} \\leftarrow \\epsilon\\text{-greedy}(\\mathbf{s}, \\hat{Q}_{\\mathbf{\\omega}})$<br>\n",
    "\t\t$\\quad\\quad$ $r, \\mathbf{s'} \\leftarrow \\text{env.step}(\\mathbf{a})$<br>\n",
    "\t\t$\\quad\\quad$ $\\color{red}{\\text{Store transition } (\\mathbf{s}, \\mathbf{a}, r, \\mathbf{s'}) \\text{ in } \\mathcal{D}}$<br>\n",
    "\t\t$\\quad\\quad$ $\\color{red}{\\text{IF } \\mathcal{D} \\text{ contains \"enough\" transitions}}$<br>\n",
    "\t\t\t$\\quad\\quad\\quad$ $\\color{red}{\\text{Sample random batch of transitions } (\\mathbf{s}_j, \\mathbf{a}_j, r_j, \\mathbf{s'}_j) \\text{ from } \\mathcal{D} \\text{ with } j=1 \\text{ to } m}$<br>\n",
    "\t\t\t$\\quad\\quad\\quad$ $\\color{red}{\\text{For each } j}$, set $y_j \\leftarrow \n",
    "\t\t\t\\begin{cases} \n",
    "\t\t\tr_j & \\text{for terminal } \\mathbf{s'}_j\\\\\n",
    "\t\t\tr_j + \\gamma \\max_{\\mathbf{a}^\\star} \\hat{Q}_{\\mathbf{\\omega}}(\\mathbf{s'}_j)_{\\mathbf{a}^\\star} & \\text{for non-terminal } \\mathbf{s'}_j\n",
    "\t\t\t\\end{cases}$<br>\n",
    "\t\t\t$\\quad\\quad\\quad$ Perform a gradient descent step on $\\left( y_j - \\hat{Q}_{\\mathbf{\\omega}}(\\mathbf{s}_j)_{\\mathbf{a}_j} \\right)^2$ with respect to the weights $\\mathbf{\\omega}$<br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{s} \\leftarrow \\mathbf{s'}$ <br>\n",
    "\t$\\quad$ <b>UNTIL</b> $\\mathbf{s}$ is final<br><br>\n",
    "<b>RETURN</b> $\\mathbf{\\omega}$ <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the Replay Buffer\n",
    "\n",
    "To incorporate experience replay into the provided naive deep value-based reinforcement learning agent definition, we need to introduce a memory buffer where experiences are stored, and then update the algorithm to sample a random batch of experiences from this buffer to update the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    A Replay Buffer.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    buffer : collections.deque\n",
    "        A double-ended queue where the transitions are stored.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    add(state: np.ndarray, action: np.int64, reward: float, next_state: np.ndarray, done: bool)\n",
    "        Add a new transition to the buffer.\n",
    "    sample(batch_size: int) -> Tuple[np.ndarray, float, float, np.ndarray, bool]\n",
    "        Sample a batch of transitions from the buffer.\n",
    "    __len__()\n",
    "        Return the current size of the buffer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int):\n",
    "        \"\"\"\n",
    "        Initializes a ReplayBuffer instance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        capacity : int\n",
    "            The maximum number of transitions that can be stored in the buffer.\n",
    "        \"\"\"\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state: np.ndarray, action: np.int64, reward: float, next_state: np.ndarray, done: bool):\n",
    "        \"\"\"\n",
    "        Add a new transition to the buffer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state : np.ndarray\n",
    "            The state vector of the added transition.\n",
    "        action : np.int64\n",
    "            The action of the added transition.\n",
    "        reward : float\n",
    "            The reward of the added transition.\n",
    "        next_state : np.ndarray\n",
    "            The next state vector of the added transition.\n",
    "        done : bool\n",
    "            The final state of the added transition.\n",
    "        \"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size: int) -> Tuple[np.ndarray, float, float, np.ndarray, bool]:\n",
    "        \"\"\"\n",
    "        Sample a batch of transitions from the buffer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size : int\n",
    "            The number of transitions to sample.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[np.ndarray, float, float, np.ndarray, bool]\n",
    "            A batch of `batch_size` transitions.\n",
    "        \"\"\"\n",
    "        # Here, `random.sample(self.buffer, batch_size)`\n",
    "        # returns a list of tuples `(state, action, reward, next_state, done)`\n",
    "        # where:\n",
    "        # - `state`  and `next_state` are numpy arrays\n",
    "        # - `action` and `reward` are floats\n",
    "        # - `done` is a boolean\n",
    "        #\n",
    "        # `states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))`\n",
    "        # generates 5 tuples `state`, `action`, `reward`, `next_state` and `done`, each having `batch_size` elements.\n",
    "        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.array(states), actions, rewards, np.array(next_states), dones\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the current size of the buffer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The current size of the buffer.\n",
    "        \"\"\"\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the training function\n",
    "\n",
    "The training function of our initial deep value-based agent needs to be modified to incorporate the use of the replay buffer effectively.\n",
    "\n",
    "1. **Store Experiences**: After the agent takes an action and receives a reward and the next state from the environment, store this experience in the replay buffer.\n",
    "\n",
    "2. **Sample Experiences**: Instead of using the most recent experience to update the agent's policy, randomly sample a batch of experiences from the replay buffer.\n",
    "\n",
    "3. **Compute Loss and Update Weights**: Use the sampled experiences to compute the loss and update the weights of the Q-Network.\n",
    "\n",
    "4. **Handle Terminal States**: If the 'done' flag of an experience is True, indicating a terminal state, make sure to adjust the target Q-value to be just the received reward. This is because there are no future rewards possible after a terminal state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parctical tips\n",
    "\n",
    "A key practical difference from the previous exercise is that we now train the Q-network on a (mini)batch of transitions rather than a single transition.  \n",
    "\n",
    "To extract $\\hat{Q}_{\\mathbf{\\omega}}(\\mathbf{s}_j)_{\\mathbf{a}_j}$ from the `q_network` output, it is essential to understand the following PyTorch concept:  \n",
    "\n",
    "- **[torch.gather](https://pytorch.org/docs/stable/generated/torch.gather.html) (or [torch.Tensor.gather](https://pytorch.org/docs/stable/generated/torch.Tensor.gather.html))**: This function retrieves values along a specified axis using an index tensor, making it particularly useful for selecting Q-values corresponding to chosen actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.1:** complete the `train_dqn1_agent` to use the replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn1_agent(env: gym.Env,\n",
    "                     q_network: torch.nn.Module,\n",
    "                     optimizer: torch.optim.Optimizer,\n",
    "                     loss_fn: Callable,\n",
    "                     epsilon_greedy: EpsilonGreedy,\n",
    "                     device: torch.device,\n",
    "                     lr_scheduler: _LRScheduler,\n",
    "                     num_episodes: int,\n",
    "                     gamma: float,\n",
    "                     batch_size: int,\n",
    "                     replay_buffer: ReplayBuffer) -> List[float]:\n",
    "    \"\"\"\n",
    "    Train the Q-network on the given environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment to train on.\n",
    "    q_network : torch.nn.Module\n",
    "        The Q-network to train.\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer to use for training.\n",
    "    loss_fn : callable\n",
    "        The loss function to use for training.\n",
    "    epsilon_greedy : EpsilonGreedy\n",
    "        The epsilon-greedy policy to use for action selection.\n",
    "    device : torch.device\n",
    "        The device to use for PyTorch computations.\n",
    "    lr_scheduler : torch.optim.lr_scheduler._LRScheduler\n",
    "        The learning rate scheduler to adjust the learning rate during training.\n",
    "    num_episodes : int\n",
    "        The number of episodes to train for.\n",
    "    gamma : float\n",
    "        The discount factor for future rewards.\n",
    "    batch_size : int\n",
    "        The size of the batch to use for training.\n",
    "    replay_buffer : ReplayBuffer\n",
    "        The replay buffer storing the experiences with their priorities.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[float]\n",
    "        A list of cumulated rewards per episode.\n",
    "    \"\"\"\n",
    "    episode_reward_list = []\n",
    "\n",
    "    for episode_index in tqdm(range(1, num_episodes)):\n",
    "        state, info = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # Get action, next_state and reward\n",
    "\n",
    "            action = epsilon_greedy(state)\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            replay_buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Update the q_network weights with a batch of experiences from the buffer\n",
    "\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "                # Convert to PyTorch tensors\n",
    "                batch_states_tensor = torch.tensor(batch_states, dtype=torch.float32, device=device)\n",
    "                batch_actions_tensor = torch.tensor(batch_actions, dtype=torch.long, device=device)\n",
    "                batch_rewards_tensor = torch.tensor(batch_rewards, dtype=torch.float32, device=device)\n",
    "                batch_next_states_tensor = torch.tensor(batch_next_states, dtype=torch.float32, device=device)\n",
    "                batch_dones_tensor = torch.tensor(batch_dones, dtype=torch.float32, device=device)\n",
    "\n",
    "                # Compute the target Q values for the batch\n",
    "                with torch.no_grad():\n",
    "                    # next_state_q_values, best_action_index = TODO... # Compute the maximum Q-values for the next states\n",
    "\n",
    "                    # targets = TODO... # Compute the target Q-values for the batch\n",
    "\n",
    "                # current_q_values = TODO... # Compute the Q-values for the current states\n",
    "\n",
    "                # Compute loss\n",
    "                loss = loss_fn(current_q_values, targets)\n",
    "\n",
    "                # Optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                lr_scheduler.step()\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        episode_reward_list.append(episode_reward)\n",
    "        epsilon_greedy.decay_epsilon()\n",
    "\n",
    "    return episode_reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "NUMBER_OF_TRAININGS = DEFAULT_NUMBER_OF_TRAININGS    # Change the default (global) value here if you want a specific number of trainings for this exercise\n",
    "dqn1_trains_result_list = [[], [], []]\n",
    "\n",
    "for train_index in range(NUMBER_OF_TRAININGS):\n",
    "\n",
    "    # Instantiate required objects\n",
    "    \n",
    "    q_network = QNetwork(state_dim, action_dim, nn_l1=128, nn_l2=128).to(device)\n",
    "    optimizer = torch.optim.AdamW(q_network.parameters(), lr=0.004, amsgrad=True)\n",
    "    #lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "    lr_scheduler = MinimumExponentialLR(optimizer, lr_decay=0.97, min_lr=0.0001)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    \n",
    "    epsilon_greedy = EpsilonGreedy(epsilon_start=0.82, epsilon_min=0.013, epsilon_decay=0.9675, env=env, q_network=q_network)\n",
    "    \n",
    "    replay_buffer = ReplayBuffer(2000)\n",
    "    \n",
    "    # Train the q-network\n",
    "\n",
    "    episode_reward_list = train_dqn1_agent(env,\n",
    "                                           q_network,\n",
    "                                           optimizer,\n",
    "                                           loss_fn,\n",
    "                                           epsilon_greedy,\n",
    "                                           device,\n",
    "                                           lr_scheduler,\n",
    "                                           num_episodes=150,\n",
    "                                           gamma=0.9,\n",
    "                                           batch_size=128,\n",
    "                                           replay_buffer=replay_buffer)\n",
    "    dqn1_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
    "    dqn1_trains_result_list[1].extend(episode_reward_list)\n",
    "    dqn1_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
    "\n",
    "dqn1_trains_result_df = pd.DataFrame(np.array(dqn1_trains_result_list).T, columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"])\n",
    "dqn1_trains_result_df[\"agent\"] = \"DQN 2013\"\n",
    "\n",
    "# Save the action-value estimation function\n",
    "\n",
    "torch.save(q_network, MODELS_DIR / \"lab1_dqn1_q_network.pth\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", estimator=None, units=\"training_index\", data=dqn1_trains_result_df,\n",
    "                height=7, aspect=2, alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", data=dqn1_trains_result_df, height=7, aspect=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trains_result_df = pd.concat([naive_trains_result_df, dqn1_trains_result_df])\n",
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", data=all_trains_result_df, height=7, aspect=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_network = torch.load(\"dqn1_q_network.pth\").to(device)\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "test_q_network_agent(env, q_network, num_episode=3)\n",
    "\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(FIGS_DIR / \"lab1_dqn1_tained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_ex3 = dqn1_trains_result_df[[\"num_episodes\", \"mean_final_episode_reward\"]].groupby(\"num_episodes\").mean().max()\n",
    "score_ex3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Implement Deep Q-Networks v2 (DQN version 2015) with *infrequent weight updates*\n",
    "\n",
    "In 2015, DeepMind further advanced the field of reinforcement learning with the publication of the paper \"Human-level control through deep reinforcement learning\" by Volodymyr Mnih and colleagues (https://www.nature.com/articles/nature14236). This work introduced the second version of Deep Q-Networks (DQN).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-53439-ep-2025-students/main/assets/lab1_dqn_nature_journal.jpg?raw=true\" width=\"200px\" />\n",
    "\n",
    "The key contribution of this paper was the introduction of a method to stabilize the learning process by infrequently updating the target weights. This technique, known as *infrequent updates of target weights*, significantly improved the stability of the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Infrequent weight updates\n",
    "\n",
    "Infrequent weight updates, also known as the use of a target network, is a technique used in Deep Q-Networks (DQN) to address the issue of learning from a moving target.\n",
    "\n",
    "In a typical DQN setup, there are two neural networks: the Q-network and the target network. The Q-network is used to predict the Q-values and is updated at every time step. The target network is used to compute the target Q-values for the update, and its weights are updated less frequently, typically every few thousand steps, by copying the weights from the Q-network.\n",
    "\n",
    "The idea behind infrequent weight updates is to stabilize the learning process by keeping the target Q-values fixed for a number of steps. This mitigates the issue of learning from a moving target, as the target Q-values remain fixed between updates.\n",
    "\n",
    "Without infrequent weight updates, both the predicted and target Q-values would change at every step, which could lead to oscillations and divergence in the learning process. By introducing a delay between updates of the target Q-values, the risk of such oscillations is reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN v2015 Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: main differences with the previous algorithm are highlighted in red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Input</b>:<br>\n",
    "\t$\\quad\\quad$ none<br>\n",
    "<b>Algorithm parameter</b>:<br>\n",
    "\t$\\quad\\quad$ discount factor $\\gamma$<br>\n",
    "\t$\\quad\\quad$ step size $\\alpha \\in (0,1]$<br>\n",
    "\t$\\quad\\quad$ small $\\epsilon > 0$<br>\n",
    "\t$\\quad\\quad$ capacity of the experience replay memory $M$<br>\n",
    "\t$\\quad\\quad$ batch size $m$<br>\n",
    "\t$\\quad\\quad$ target network update frequency $\\color{red}{\\tau}$<br><br>\n",
    "\n",
    "<b>Initialize</b> replay memory $\\mathcal{D}$ to capacity $M$<br>\n",
    "<b>Initialize</b> action-value function $\\hat{Q}_{\\mathbf{\\omega_1}}$ with random weights $\\mathbf{\\omega_1}$<br>\n",
    "<b>Initialize</b> target action-value function $\\hat{Q}_{\\mathbf{\\omega_2}}$ with weights $\\color{red}{\\mathbf{\\omega_2} = \\mathbf{\\omega_1}}$<br><br>\n",
    "\n",
    "<b>FOR EACH</b> episode<br>\n",
    "\t$\\quad$ $\\mathbf{s} \\leftarrow \\text{env.reset}()$<br>\n",
    "\t$\\quad$ <b>DO</b> <br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{a} \\leftarrow \\epsilon\\text{-greedy}(\\mathbf{s}, \\hat{Q}_{\\mathbf{\\omega_1}})$<br>\n",
    "\t\t$\\quad\\quad$ $r, \\mathbf{s'} \\leftarrow \\text{env.step}(\\mathbf{a})$<br>\n",
    "\t\t$\\quad\\quad$ Store transition $(\\mathbf{s}, \\mathbf{a}, r, \\mathbf{s'})$ in $\\mathcal{D}$<br>\n",
    "\t\t$\\quad\\quad$ If $\\mathcal{D}$ contains \"enough\" transitions<br>\n",
    "\t\t\t$\\quad\\quad\\quad$ Sample random batch of transitions $(\\mathbf{s}_j, \\mathbf{a}_j, r_j, \\mathbf{s'}_j)$ from $\\mathcal{D}$ with $j=1$ to $m$<br>\n",
    "\t\t\t$\\quad\\quad\\quad$ For each $j$, set $y_j = \n",
    "\t\t\t\\begin{cases} \n",
    "\t\t\tr_j & \\text{for terminal } \\mathbf{s'}_j\\\\\n",
    "\t\t\tr_j + \\gamma \\max_{\\mathbf{a}^\\star} \\hat{Q}_{\\mathbf{\\omega_{\\color{red}{2}}}} (\\mathbf{s'}_j)_{\\mathbf{a}^\\star} & \\text{for non-terminal } \\mathbf{s'}_j\n",
    "\t\t\t\\end{cases}$<br>\n",
    "\t\t\t$\\quad\\quad\\quad$ Perform a gradient descent step on $\\left( y_j - \\hat{Q}_{\\mathbf{\\omega_1}}(\\mathbf{s}_j)_{\\mathbf{a}_j} \\right)^2$ with respect to the weights $\\mathbf{\\omega_1}$<br>\n",
    "\t\t\t$\\quad\\quad\\quad$ Every $\\color{red}{\\tau}$ steps reset $\\hat{Q}_{\\mathbf{\\omega_2}}$ to $\\hat{Q}_{\\mathbf{\\omega_1}}$, i.e., set $\\color{red}{\\mathbf{\\omega_2} \\leftarrow \\mathbf{\\omega_1}}$<br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{s} \\leftarrow \\mathbf{s'}$ <br>\n",
    "\t$\\quad$ <b>UNTIL</b> $\\mathbf{s}$ is final<br><br>\n",
    "<b>RETURN</b> $\\mathbf{\\omega_1}$ <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the training function\n",
    "\n",
    "To incorporate the use of infrequent weight updates in the training function, you would need to make the following modifications:\n",
    "\n",
    "1. **Update the Target Network Infrequently**: Instead of updating the weights of the target network at every time step, update them less frequently, for example, every few thousand steps. The weights of the target network are updated by copying the weights from the Q-network.\n",
    "\n",
    "2. **Compute Target Q-values with the Target Network**: When computing the target Q-values for the update, use the target network instead of the Q-network. This ensures that the target Q-values remain fixed between updates, which stabilizes the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parctical tips\n",
    "\n",
    "To complete this exercise, you need to be familiar with the following PyTorch concepts.\n",
    "\n",
    "**Copying the weights and biases from one neural network to another**\n",
    "\n",
    "To transfer the parameters from `model1` to `model2`, use the following command:\n",
    "\n",
    "```python\n",
    "model2.load_state_dict(model1.state_dict())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.1:** complete the `train_dqn2_agent` to apply infrequent weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn2_agent(env: gym.Env,\n",
    "                     q_network: torch.nn.Module,\n",
    "                     target_q_network: torch.nn.Module,\n",
    "                     optimizer: torch.optim.Optimizer,\n",
    "                     loss_fn: Callable,\n",
    "                     epsilon_greedy: EpsilonGreedy,\n",
    "                     device: torch.device,\n",
    "                     lr_scheduler: _LRScheduler,\n",
    "                     num_episodes: int,\n",
    "                     gamma: float,\n",
    "                     batch_size: int,\n",
    "                     replay_buffer: ReplayBuffer,\n",
    "                     target_q_network_sync_period: int) -> List[float]:\n",
    "    \"\"\"\n",
    "    Train the Q-network on the given environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment to train on.\n",
    "    q_network : torch.nn.Module\n",
    "        The Q-network to train.\n",
    "    target_q_network : torch.nn.Module\n",
    "        The target Q-network to use for estimating the target Q-values.\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer to use for training.\n",
    "    loss_fn : callable\n",
    "        The loss function to use for training.\n",
    "    epsilon_greedy : EpsilonGreedy\n",
    "        The epsilon-greedy policy to use for action selection.\n",
    "    device : torch.device\n",
    "        The device to use for PyTorch computations.\n",
    "    lr_scheduler : torch.optim.lr_scheduler._LRScheduler\n",
    "        The learning rate scheduler to adjust the learning rate during training.\n",
    "    num_episodes : int\n",
    "        The number of episodes to train for.\n",
    "    gamma : float\n",
    "        The discount factor for future rewards.\n",
    "    batch_size : int\n",
    "        The size of the batch to use for training.\n",
    "    replay_buffer : ReplayBuffer\n",
    "        The replay buffer storing the experiences with their priorities.\n",
    "    target_q_network_sync_period : int\n",
    "        The number of episodes after which the target Q-network should be updated with the weights of the Q-network.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[float]\n",
    "        A list of cumulated rewards per episode.\n",
    "    \"\"\"\n",
    "    iteration = 0\n",
    "    episode_reward_list = []\n",
    "\n",
    "    for episode_index in tqdm(range(1, num_episodes)):\n",
    "        state, info = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # Get action, next_state and reward\n",
    "\n",
    "            action = epsilon_greedy(state)\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            replay_buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Update the q_network weights with a batch of experiences from the buffer\n",
    "\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "                # Convert to PyTorch tensors\n",
    "                batch_states_tensor = torch.tensor(batch_states, dtype=torch.float32, device=device)\n",
    "                batch_actions_tensor = torch.tensor(batch_actions, dtype=torch.long, device=device)\n",
    "                batch_rewards_tensor = torch.tensor(batch_rewards, dtype=torch.float32, device=device)\n",
    "                batch_next_states_tensor = torch.tensor(batch_next_states, dtype=torch.float32, device=device)\n",
    "                batch_dones_tensor = torch.tensor(batch_dones, dtype=torch.float32, device=device)\n",
    "\n",
    "                # Compute the target Q values for the batch\n",
    "                with torch.no_grad():\n",
    "                    # next_state_q_values, best_action_index = TODO...\n",
    "                    \n",
    "                    # targets = TODO...\n",
    "\n",
    "                # current_q_values = TODO...\n",
    "\n",
    "                # Compute loss\n",
    "                loss = loss_fn(current_q_values, targets)\n",
    "\n",
    "                # Optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                lr_scheduler.step()\n",
    "\n",
    "            # Update the target q-network\n",
    "\n",
    "            # TODO:...  # Every episodes (e.g., every `target_q_network_sync_period` episodes), the weights of the target network are updated with the weights of the Q-network\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        episode_reward_list.append(episode_reward)\n",
    "        epsilon_greedy.decay_epsilon()\n",
    "\n",
    "    return episode_reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train it\n",
    "\n",
    "In order to test this new implementation, we needs de adapt the following cell to instantiate and initialize the two neural networks.\n",
    "\n",
    "**Task 4.2:** complete the following cell to make the two Q-Networks. Initialize a target network that has the same architecture as the Q-network. The weights of the target network are initially copied from the Q-network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# NUMBER_OF_TRAININGS = 20\n",
    "dqn2_trains_result_list = [[], [], []]\n",
    "\n",
    "for train_index in range(NUMBER_OF_TRAININGS):\n",
    "\n",
    "    # Instantiate required objects\n",
    "\n",
    "    # q_network = TODO...\n",
    "\n",
    "    # target_q_network = TODO... # The target Q-network is used to compute the target Q-values for the loss function\n",
    "\n",
    "    # TODO... # Initialize the target Q-network with the same weights as the Q-network (c.f. the \"Practical tips\" section of the exercise)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(q_network.parameters(), lr=0.004, amsgrad=True)\n",
    "    #lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "    lr_scheduler = MinimumExponentialLR(optimizer, lr_decay=0.97, min_lr=0.0001)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    epsilon_greedy = EpsilonGreedy(epsilon_start=0.82, epsilon_min=0.013, epsilon_decay=0.9675, env=env, q_network=q_network)\n",
    "\n",
    "    replay_buffer = ReplayBuffer(2000)\n",
    "\n",
    "    # Train the q-network\n",
    "\n",
    "    episode_reward_list = train_dqn2_agent(env,\n",
    "                                           q_network,\n",
    "                                           target_q_network,\n",
    "                                           optimizer,\n",
    "                                           loss_fn,\n",
    "                                           epsilon_greedy,\n",
    "                                           device,\n",
    "                                           lr_scheduler,\n",
    "                                           num_episodes=150,\n",
    "                                           gamma=0.9,\n",
    "                                           batch_size=128,\n",
    "                                           replay_buffer=replay_buffer,\n",
    "                                           target_q_network_sync_period=30)\n",
    "    dqn2_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
    "    dqn2_trains_result_list[1].extend(episode_reward_list)\n",
    "    dqn2_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
    "\n",
    "dqn2_trains_result_df = pd.DataFrame(np.array(dqn2_trains_result_list).T, columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"])\n",
    "dqn2_trains_result_df[\"agent\"] = \"DQN 2015\"\n",
    "\n",
    "# Save the action-value estimation function\n",
    "\n",
    "torch.save(q_network, MODELS_DIR / \"lab1_dqn2_q_network.pth\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", estimator=None, units=\"training_index\", data=dqn2_trains_result_df,\n",
    "                height=7, aspect=2, alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", data=dqn2_trains_result_df, height=7, aspect=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trains_result_df = pd.concat([naive_trains_result_df, dqn1_trains_result_df, dqn2_trains_result_df])\n",
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", data=all_trains_result_df, height=7, aspect=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_network = torch.load(\"dqn2_q_network.pth\").to(device)\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "test_q_network_agent(env, q_network, num_episode=3)\n",
    "\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(FIGS_DIR / \"lab1_dqn2_tained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_ex4 = dqn2_trains_result_df[[\"num_episodes\", \"mean_final_episode_reward\"]].groupby(\"num_episodes\").mean().max()\n",
    "score_ex4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Double Deep Q-Network (DDQN)\n",
    "\n",
    "Hado Van Hasselt et al. introduced Double Deep Q-Networks in the publication \"Deep reinforcement learning with Double Q-Learning\" in 2016 (https://arxiv.org/abs/1509.06461).\n",
    "\n",
    "Double Deep Q-Networks (DDQN) is an enhancement over the standard Deep Q-Network (DQN). It was designed to reduce the overestimation of action values that can occur in DQN. The fundamental concept behind DDQN is the separation of action selection from their evaluation to reduce the overestimation of action values in DQN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDQN Algorithm\n",
    "\n",
    "Note: main differences with the previous algorithm are highlighted in red.\n",
    "\n",
    "<b>Input</b>:<br>\n",
    "\t$\\quad\\quad$ none<br>\n",
    "<b>Algorithm parameter</b>:<br>\n",
    "\t$\\quad\\quad$ discount factor $\\gamma$<br>\n",
    "\t$\\quad\\quad$ step size $\\alpha \\in (0,1]$<br>\n",
    "\t$\\quad\\quad$ small $\\epsilon > 0$<br>\n",
    "\t$\\quad\\quad$ capacity of the experience replay memory $M$<br>\n",
    "\t$\\quad\\quad$ batch size $m$<br>\n",
    "\t$\\quad\\quad$ target network update frequency $\\tau$<br><br>\n",
    "\n",
    "<b>Initialize</b> replay memory $\\mathcal{D}$ to capacity $M$<br>\n",
    "<b>Initialize</b> action-value function $\\hat{Q}$ with random weights $\\mathbf{\\omega_1}$<br>\n",
    "<b>Initialize</b> target action-value function $\\hat{Q}$ with weights $\\mathbf{\\omega_2} = \\mathbf{\\omega_1}$<br><br>\n",
    "\n",
    "<b>FOR EACH</b> episode<br>\n",
    "\t$\\quad$ $\\mathbf{s} \\leftarrow \\text{env.reset}()$<br>\n",
    "\t$\\quad$ <b>DO</b> <br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{a} \\leftarrow \\epsilon\\text{-greedy}(\\mathbf{s}, \\hat{Q}_{\\mathbf{\\omega_1}})$<br>\n",
    "\t\t$\\quad\\quad$ $r, \\mathbf{s'} \\leftarrow \\text{env.step}(\\mathbf{a})$<br>\n",
    "\t\t$\\quad\\quad$ Store transition $(\\mathbf{s}, \\mathbf{a}, r, \\mathbf{s'})$ in $\\mathcal{D}$<br>\n",
    "\t\t$\\quad\\quad$ If $\\mathcal{D}$ contains \"enough\" transitions<br>\n",
    "\t\t\t$\\quad\\quad\\quad$ Sample random batch of transitions $(\\mathbf{s}_j, \\mathbf{a}_j, r_j, \\mathbf{s'}_j)$ from $\\mathcal{D}$ with $j=1$ to $m$<br>\n",
    "\t\t\t$\\quad\\quad\\quad$ For each $j$, set $\\color{red}{\\mathbf{a}^{\\star} \\leftarrow \\arg\\max_{\\mathbf{a}} \\hat{Q}_{\\mathbf{\\omega_1}}(\\mathbf{s'}_j)_{\\mathbf{a}}}$<br>\n",
    "\t\t\t$\\quad\\quad\\quad$ For each $j$, set $y_j \\leftarrow \n",
    "\t\t\t\\begin{cases} \n",
    "\t\t\tr_j & \\text{for terminal } \\mathbf{s'}_j\\\\\n",
    "\t\t\tr_j + \\gamma \\hat{Q}_{\\mathbf{\\omega_2}} (\\mathbf{s'}_j)_{\\color{red}{\\mathbf{a}^\\star}} & \\text{for non-terminal } \\mathbf{s'}_j\n",
    "\t\t\t\\end{cases}$<br>\n",
    "\t\t\t$\\quad\\quad\\quad$ Perform a gradient descent step on $\\left( y_j - \\hat{Q}_{\\mathbf{\\omega_1}}(\\mathbf{s}_j)_{\\mathbf{a}_j} \\right)^2$ with respect to the weights $\\mathbf{\\omega_1}$<br>\n",
    "\t\t\t$\\quad\\quad\\quad$ Every $\\tau$ steps reset $\\hat{Q}_{\\mathbf{\\omega_2}}$ to $\\hat{Q}_{\\mathbf{\\omega_1}}$, i.e., set $\\mathbf{\\omega_2} \\leftarrow \\mathbf{\\omega_1}$<br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{s} \\leftarrow \\mathbf{s'}$ <br>\n",
    "\t$\\quad$ <b>UNTIL</b> $\\mathbf{s}$ is final<br><br>\n",
    "<b>RETURN</b> $\\mathbf{\\omega_1}$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Implement DDQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the training function\n",
    "\n",
    "Switching from a Deep Q-Network (DQN) to a Double Deep Q-Network (DDQN) involves a key modification in the way the Q-value update is performed during training. \n",
    "\n",
    "In DQN, the Q-value update is done using the maximum Q-value for the next state from the target network. However, this can lead to an overestimation of Q-values because it always uses the maximum estimate.\n",
    "\n",
    "DDQN addresses this by decoupling the selection of the action from the evaluation of that action. In DDQN, the Q-network is used to select what the next action is, and the target network is used to evaluate the Q-value of taking that action at the next state.\n",
    "\n",
    "**Task 4.2:** complete the `train_ddqn_agent` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ddqn_agent(env: gym.Env,\n",
    "                     q_network: torch.nn.Module,\n",
    "                     target_q_network: torch.nn.Module,\n",
    "                     optimizer: torch.optim.Optimizer,\n",
    "                     loss_fn: Callable,\n",
    "                     epsilon_greedy: EpsilonGreedy,\n",
    "                     device: torch.device,\n",
    "                     lr_scheduler: _LRScheduler,\n",
    "                     num_episodes: int,\n",
    "                     gamma: float,\n",
    "                     batch_size: int,\n",
    "                     replay_buffer: ReplayBuffer,\n",
    "                     target_q_network_sync_period: int) -> List[float]:\n",
    "    \"\"\"\n",
    "    Train the Q-network on the given environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment to train on.\n",
    "    q_network : torch.nn.Module\n",
    "        The Q-network to train.\n",
    "    target_q_network : torch.nn.Module\n",
    "        The target Q-network to use for estimating the target Q-values.\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer to use for training.\n",
    "    loss_fn : callable\n",
    "        The loss function to use for training.\n",
    "    epsilon_greedy : EpsilonGreedy\n",
    "        The epsilon-greedy policy to use for action selection.\n",
    "    device : torch.device\n",
    "        The device to use for PyTorch computations.\n",
    "    lr_scheduler : torch.optim.lr_scheduler._LRScheduler\n",
    "        The learning rate scheduler to adjust the learning rate during training.\n",
    "    num_episodes : int\n",
    "        The number of episodes to train for.\n",
    "    gamma : float\n",
    "        The discount factor for future rewards.\n",
    "    batch_size : int\n",
    "        The size of the batch to use for training.\n",
    "    replay_buffer : ReplayBuffer\n",
    "        The replay buffer storing the experiences with their priorities.\n",
    "    target_q_network_sync_period : int\n",
    "        The number of episodes after which the target Q-network should be updated with the weights of the Q-network.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[float]\n",
    "        A list of cumulated rewards per episode.\n",
    "    \"\"\"\n",
    "    iteration = 0\n",
    "    episode_reward_list = []\n",
    "\n",
    "    for episode_index in tqdm(range(1, num_episodes)):\n",
    "        state, info = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # GET ACTION, NEXT_STATE AND REWARD ###########\n",
    "\n",
    "            action = epsilon_greedy(state)\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            replay_buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "            # UPDATE THE Q_NETWORK WEIGHTS WITH A BATCH OF EXPERIENCES FROM THE BUFFER\n",
    "\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "                # Convert to PyTorch tensors\n",
    "                batch_states_tensor = torch.tensor(batch_states, dtype=torch.float32, device=device)\n",
    "                batch_actions_tensor = torch.tensor(batch_actions, dtype=torch.long, device=device)\n",
    "                batch_rewards_tensor = torch.tensor(batch_rewards, dtype=torch.float32, device=device)\n",
    "                batch_next_states_tensor = torch.tensor(batch_next_states, dtype=torch.float32, device=device)\n",
    "                batch_dones_tensor = torch.tensor(batch_dones, dtype=torch.float32, device=device)\n",
    "\n",
    "                # Compute the target Q values for the batch\n",
    "                with torch.no_grad():\n",
    "                    # best_actions = TODO... # First, select the best action using the online network (q_network)\n",
    "\n",
    "                    # next_state_q_values = TODO... # Then, use the target network (target_q_network) to get the Q-value for these actions\n",
    "\n",
    "                    # targets = TODO... # The targets for the batch are the rewards plus the discounted maximum Q-values obtained from the next states.\n",
    "\n",
    "\n",
    "                # Compute the current Q values for the batch.\n",
    "                # \n",
    "                # The expression `gather(dim=1, index=batch_actions_tensor.unsqueeze(-1)).squeeze(-1)` is used to select specific elements from the tensor of Q-values returned by the Q-network.\n",
    "                # \n",
    "                # Here's a breakdown of the following line of code:\n",
    "                # - `q_network(batch_states_tensor)`:\n",
    "                #   This is passing a batch of states through the Q-network.\n",
    "                #   For each state, this outputs the Q-value for each possible action.\n",
    "                #   Thus, `q_network(batch_states_tensor)` returns a tensor of shape (batch_size, action_dim).\n",
    "                # \n",
    "                # - `gather(dim=1, index=batch_actions_tensor.unsqueeze(-1))`:\n",
    "                #   This is selecting the Q-values corresponding to the actions that were actually taken.\n",
    "                #   The `gather` function is used to select elements from a tensor using an index.\n",
    "                #   In this case, the index is `batch_actions_tensor.unsqueeze(-1)`, which is a tensor of the actions that were taken.\n",
    "                #   The `unsqueeze(-1)` function is used to add an extra dimension to the tensor, which is necessary for the `gather` function.\n",
    "                # \n",
    "                # - `squeeze(-1)`:\n",
    "                #   This is removing the extra dimension that was added by `unsqueeze(-1)`.\n",
    "                #   The `squeeze` function is used to remove dimensions of size 1 from a tensor.\n",
    "                #\n",
    "                # So, the entire expression is selecting the Q-values of the actions that were actually taken from the tensor of all Q-values,\n",
    "                # and returning a tensor of these selected Q-values.\n",
    "                current_q_values = q_network(batch_states_tensor).gather(dim=1, index=batch_actions_tensor.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = loss_fn(current_q_values, targets)\n",
    "\n",
    "                # Optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                lr_scheduler.step()\n",
    "\n",
    "            # UPDATE THE TARGET Q-NETWORK #################\n",
    "\n",
    "            # Every few training steps (e.g., every 100 steps), the weights of the target network are updated with the weights of the Q-network\n",
    "\n",
    "            if iteration % target_q_network_sync_period == 0:\n",
    "                target_q_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        episode_reward_list.append(episode_reward)\n",
    "        epsilon_greedy.decay_epsilon()\n",
    "\n",
    "    return episode_reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "NUMBER_OF_TRAININGS = DEFAULT_NUMBER_OF_TRAININGS    # Change the default (global) value here if you want a specific number of trainings for this exercice\n",
    "ddqn_trains_result_list = [[], [], []]\n",
    "\n",
    "for train_index in range(NUMBER_OF_TRAININGS):\n",
    "\n",
    "    # Instantiate required objects\n",
    "\n",
    "    q_network = QNetwork(state_dim, action_dim, nn_l1=128, nn_l2=128).to(device)\n",
    "    target_q_network = QNetwork(state_dim, action_dim, nn_l1=128, nn_l2=128).to(device) # The target Q-network is used to compute the target Q-values for the loss function\n",
    "    target_q_network.load_state_dict(q_network.state_dict()) # Initialize the target Q-network with the same weights as the Q-network\n",
    "\n",
    "    optimizer = torch.optim.AdamW(q_network.parameters(), lr=0.004, amsgrad=True)\n",
    "    #lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "    lr_scheduler = MinimumExponentialLR(optimizer, lr_decay=0.97, min_lr=0.0001)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    epsilon_greedy = EpsilonGreedy(epsilon_start=0.82, epsilon_min=0.013, epsilon_decay=0.9675, env=env, q_network=q_network)\n",
    "\n",
    "    replay_buffer = ReplayBuffer(2000)\n",
    "\n",
    "    # Train the q-network\n",
    "\n",
    "    episode_reward_list = train_ddqn_agent(env,\n",
    "                                           q_network,\n",
    "                                           target_q_network,\n",
    "                                           optimizer,\n",
    "                                           loss_fn,\n",
    "                                           epsilon_greedy,\n",
    "                                           device,\n",
    "                                           lr_scheduler,\n",
    "                                           num_episodes=150,\n",
    "                                           gamma=0.9,\n",
    "                                           batch_size=128,\n",
    "                                           replay_buffer=replay_buffer,\n",
    "                                           target_q_network_sync_period=30)\n",
    "    ddqn_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
    "    ddqn_trains_result_list[1].extend(episode_reward_list)\n",
    "    ddqn_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
    "\n",
    "ddqn_trains_result_df = pd.DataFrame(np.array(ddqn_trains_result_list).T, columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"])\n",
    "ddqn_trains_result_df[\"agent\"] = \"DDQN\"\n",
    "\n",
    "# SAVE THE ACTION-VALUE ESTIMATION FUNCTION\n",
    "\n",
    "torch.save(q_network, MODELS_DIR / \"lab1_ddqn_q_network.pth\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", estimator=None, units=\"training_index\", data=ddqn_trains_result_df,\n",
    "                height=7, aspect=2, alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", data=ddqn_trains_result_df, height=7, aspect=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trains_result_df = pd.concat([naive_trains_result_df, dqn1_trains_result_df, dqn2_trains_result_df, ddqn_trains_result_df])\n",
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", data=all_trains_result_df, height=7, aspect=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_network = torch.load(\"ddqn_q_network.pth\").to(device)\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "test_q_network_agent(env, q_network, num_episode=3)\n",
    "\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(FIGS_DIR / \"lab1_ddqn_tained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental results\n",
    "\n",
    "**Task 5.2:** What do you observe? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Prioritized Experience Replay (PEX or PER)\n",
    "\n",
    "Prioritized Experience Replay (PER) is an enhancement to the traditional experience replay mechanism used in Deep Q-Networks (DQNs) and other reinforcement learning algorithms. In traditional experience replay, experiences (or transitions) are stored in a replay buffer, and mini-batches are sampled uniformly at random from this buffer to update the agent's Q-network. This means every experience has the same probability of being chosen, regardless of its significance to the learning process.\n",
    "\n",
    "Prioritized Experience Replay (PER) is a technique that modifies the standard experience replay by more frequently replaying experiences that have a high expected learning progress, as measured by their temporal-difference (TD) error.\n",
    "\n",
    "Prioritized Experience Replay was introduced by Tom Schaul et al. in the paper \"Prioritised experience replay\" in 2016 (https://arxiv.org/abs/1511.05952)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### DQN with PER algorithm\n",
    "\n",
    "The DQN with Prioritized Experience Replay (PER) algorithm is detailed in page 5 of the paper \"Prioritised experience replay\" (https://arxiv.org/abs/1511.05952)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Implement DQN with PER\n",
    "\n",
    "To switch from a DQN to a DQN with Prioritized Experience Replay (PER), you need to make a few modifications to the *DQN 2015* agent:\n",
    "\n",
    "1. **Experience Replay Buffer**: Replace the standard experience replay buffer with a prioritized one. This buffer should store experiences with a priority value that is updated after each learning step. The priority value is usually the absolute TD error plus a small constant to avoid experiences having zero probability of being chosen.\n",
    "2. **Sampling Method**: Change the sampling method from uniform to prioritized sampling. This means experiences with higher priority values have a higher probability of being chosen for learning.\n",
    "3. **Loss Function**: Modify the loss function to include importance sampling weights. These weights compensate for the bias introduced by the non-uniform sampling. The weight of each sampled experience is the inverse of its probability of being chosen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the PrioritizedReplayBuffer\n",
    "\n",
    "**Task 6.1:** implement the `PrioritizedReplayBuffer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer:\n",
    "    \"\"\"\n",
    "    Implements a Prioritized Experience Replay buffer as described in the paper\n",
    "    \"Prioritized Experience Replay\" (https://arxiv.org/abs/1511.05952).\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    buffer : Deque[Tuple[float, float, float, float, float]]\n",
    "        The replay buffer storing the experiences.\n",
    "    priorities : Deque[float]\n",
    "        The priorities of the experiences in the buffer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int):\n",
    "        \"\"\"\n",
    "        Initialize the replay buffer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        capacity : int\n",
    "            The maximum number of experiences the buffer can hold.\n",
    "        \"\"\"\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "        self.priorities = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state: float, action: float, reward: float, next_state: float, done: float):\n",
    "        \"\"\"\n",
    "        Add an experience to the buffer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state : float\n",
    "            The current state.\n",
    "        action : float\n",
    "            The action taken.\n",
    "        reward : float\n",
    "            The reward received.\n",
    "        next_state : float\n",
    "            The next state.\n",
    "        done : float\n",
    "            Whether the episode has ended.\n",
    "        \"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "        self.priorities.append(max(self.priorities, default=1))\n",
    "\n",
    "    def sample(self, batch_size: int, alpha: float = 0.6, beta: float = 0.4) -> Tuple[List[Tuple[float, float, float, float, float]], np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Sample a batch of experiences from the buffer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size : int\n",
    "            The number of experiences to sample.\n",
    "        alpha : float, optional\n",
    "            The exponent that determines how much prioritization is used.\n",
    "        beta : float, optional\n",
    "            The exponent that determines how much importance sampling is used.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[List[Tuple[float, float, float, float, float]], np.ndarray, np.ndarray]\n",
    "            The sampled experiences, the indices of the sampled experiences, and the importance sampling weights.\n",
    "        \"\"\"\n",
    "\n",
    "        # priorities = TODO... # Calculate priorities\n",
    "\n",
    "        # indices = TODO...\n",
    "        # experiences = TODO... # Sample experiences\n",
    "\n",
    "        # weights = TODO... # Calculate weights\n",
    "\n",
    "        return experiences, indices, weights\n",
    "\n",
    "    def update_priorities(self, indices: np.ndarray, errors: List[float], offset: float = 0.1):\n",
    "        \"\"\"\n",
    "        Update the priorities of the sampled experiences.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        indices : np.ndarray\n",
    "            The indices of the sampled experiences.\n",
    "        errors : List[float]\n",
    "            The new TD errors for the sampled experiences.\n",
    "        offset : float, optional\n",
    "            A small constant to ensure the priorities are strictly positive.\n",
    "        \"\"\"\n",
    "        for i, error in zip(indices, errors):\n",
    "            self.priorities[i] = error + offset\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Return the current size of the buffer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The current size of the buffer.\n",
    "        \"\"\"\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the training function\n",
    "\n",
    "**Task 6.2:** complete the `train_dqn_per_agent` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn_per_agent(env: gym.Env,\n",
    "                        q_network: torch.nn.Module,\n",
    "                        target_q_network: torch.nn.Module,\n",
    "                        optimizer: torch.optim.Optimizer,\n",
    "                        loss_fn: Callable,\n",
    "                        epsilon_greedy: EpsilonGreedy,\n",
    "                        device: torch.device,\n",
    "                        lr_scheduler: _LRScheduler,\n",
    "                        num_episodes: int,\n",
    "                        gamma: float,\n",
    "                        batch_size: int,\n",
    "                        replay_buffer: PrioritizedReplayBuffer,  # Use a Prioritized Replay Buffer\n",
    "                        target_q_network_sync_period: int,\n",
    "                        alpha: float = 0.6,  # Alpha parameter for PER\n",
    "                        beta: float = 0.4):  # Beta parameter for PER\n",
    "    \"\"\"\n",
    "    Train the Q-network on the given environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment to train on.\n",
    "    q_network : torch.nn.Module\n",
    "        The Q-network to train.\n",
    "    target_q_network : torch.nn.Module\n",
    "        The target Q-network to use for estimating the target Q-values.\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer to use for training.\n",
    "    loss_fn : callable\n",
    "        The loss function to use for training.\n",
    "    epsilon_greedy : EpsilonGreedy\n",
    "        The epsilon-greedy policy to use for action selection.\n",
    "    device : torch.device\n",
    "        The device to use for PyTorch computations.\n",
    "    lr_scheduler : torch.optim.lr_scheduler._LRScheduler\n",
    "        The learning rate scheduler to adjust the learning rate during training.\n",
    "    num_episodes : int\n",
    "        The number of episodes to train for.\n",
    "    gamma : float\n",
    "        The discount factor for future rewards.\n",
    "    batch_size : int\n",
    "        The size of the batch to use for training.\n",
    "    replay_buffer : PrioritizedReplayBuffer\n",
    "        The replay buffer storing the experiences with their priorities.\n",
    "    target_q_network_sync_period : int\n",
    "        The number of episodes after which the target Q-network should be updated with the weights of the Q-network.\n",
    "    alpha : float\n",
    "        The exponent that determines how much prioritization is used when sampling from the replay buffer.\n",
    "    beta : float\n",
    "        The exponent that determines how much importance sampling is used to adjust the loss function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[float]\n",
    "        A list of cumulated rewards per episode.\n",
    "    \"\"\"\n",
    "    iteration = 0\n",
    "    episode_reward_list = []\n",
    "\n",
    "    for episode_index in tqdm(range(1, num_episodes)):\n",
    "        state, info = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # GET ACTION, NEXT_STATE AND REWARD ###########\n",
    "\n",
    "            action = epsilon_greedy(state)\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            replay_buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "            # UPDATE THE Q_NETWORK WEIGHTS WITH A BATCH OF EXPERIENCES FROM THE BUFFER\n",
    "\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                # Sample experiences and importance-sampling weights from the buffer\n",
    "\n",
    "                # experiences, indices, weights = TODO...\n",
    "                batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = zip(*experiences)\n",
    "\n",
    "                # Convert to PyTorch tensors\n",
    "                batch_states_tensor = torch.tensor(batch_states, dtype=torch.float32, device=device)\n",
    "                batch_actions_tensor = torch.tensor(batch_actions, dtype=torch.long, device=device)\n",
    "                batch_rewards_tensor = torch.tensor(batch_rewards, dtype=torch.float32, device=device)\n",
    "                batch_next_states_tensor = torch.tensor(batch_next_states, dtype=torch.float32, device=device)\n",
    "                batch_dones_tensor = torch.tensor(batch_dones, dtype=torch.float32, device=device)\n",
    "                weights_tensor = torch.tensor(weights, dtype=torch.float32, device=device)\n",
    "\n",
    "                # Compute the target Q values for the batch\n",
    "                with torch.no_grad():\n",
    "                    # Here's a breakdown of the next line of code:\n",
    "                    # - `q_network(batch_next_states_t)`:\n",
    "                    #   This is passing a batch of \"next states\" through the Q-network.\n",
    "                    #   This outputs the Q-value for each possible action, a tensor of shape (batch_size, action_dim).\n",
    "                    #\n",
    "                    #  - `.max(dim=1)`:\n",
    "                    #   This is finding the maximum Q-value for each state in the batch.\n",
    "                    #   The dim=1 argument specifies that the maximum should be taken over the action dimension.\n",
    "                    #\n",
    "                    # The max() function in PyTorch returns a tuple containing two tensors: the maximum values and the indices where these maximum values were found.\n",
    "                    # In the next lines of code, we will just use the first tensor (the maximum values) and ignoring the second tensor (the indices).\n",
    "                    next_state_q_values, best_action_index = target_q_network(batch_next_states_tensor).max(dim=1)\n",
    "\n",
    "                    # The targets for the batch are the rewards plus the discounted maximum Q-values obtained from the next states.\n",
    "                    # The expression `(1 - batch_dones_tensor)` is used to handle the end of episodes.\n",
    "                    # The `batch_dones_tensor` indicates whether each state in the batch is a terminal state (i.e., the end of an episode).\n",
    "                    # If a state is a terminal state, the corresponding value in `batch_dones_tensor` is 1, otherwise it's 0.\n",
    "                    # The Q-value of a terminal state is defined to be 0. Therefore, when calculating the target Q-values,\n",
    "                    # we don't want to include the Q-value of the next state if the current state is a terminal state.\n",
    "                    # This is achieved by multiplying `next_state_q_values` by `(1 - batch_dones_tensor)`.\n",
    "                    # If the state is a terminal state, this expression becomes 0 and the Q-value of the next state is effectively ignored.\n",
    "                    # If the state is not a terminal state, this expression is 1 and the Q-value of the next state is included in the calculation.\n",
    "                    targets = batch_rewards_tensor + gamma * next_state_q_values * (1 - batch_dones_tensor)\n",
    "\n",
    "                # Compute the current Q values for the batch.\n",
    "                # \n",
    "                # The expression `gather(dim=1, index=batch_actions_tensor.unsqueeze(-1)).squeeze(-1)` is used to select specific elements from the tensor of Q-values returned by the Q-network.\n",
    "                # \n",
    "                # Here's a breakdown of the following line of code:\n",
    "                # - `q_network(batch_states_tensor)`:\n",
    "                #   This is passing a batch of states through the Q-network.\n",
    "                #   For each state, this outputs the Q-value for each possible action.\n",
    "                #   Thus, `q_network(batch_states_tensor)` returns a tensor of shape (batch_size, action_dim).\n",
    "                # \n",
    "                # - `gather(dim=1, index=batch_actions_tensor.unsqueeze(-1))`:\n",
    "                #   This is selecting the Q-values corresponding to the actions that were actually taken.\n",
    "                #   The `gather` function is used to select elements from a tensor using an index.\n",
    "                #   In this case, the index is `batch_actions_tensor.unsqueeze(-1)`, which is a tensor of the actions that were taken.\n",
    "                #   The `unsqueeze(-1)` function is used to add an extra dimension to the tensor, which is necessary for the `gather` function.\n",
    "                # \n",
    "                # - `squeeze(-1)`:\n",
    "                #   This is removing the extra dimension that was added by `unsqueeze(-1)`.\n",
    "                #   The `squeeze` function is used to remove dimensions of size 1 from a tensor.\n",
    "                #\n",
    "                # So, the entire expression is selecting the Q-values of the actions that were actually taken from the tensor of all Q-values,\n",
    "                # and returning a tensor of these selected Q-values.\n",
    "                current_q_values = q_network(batch_states_tensor).gather(dim=1, index=batch_actions_tensor.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "                # Compute loss with importance-sampling weights\n",
    "\n",
    "                # loss = TODO...\n",
    "\n",
    "                # Optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                lr_scheduler.step()\n",
    "\n",
    "                # Update priorities in the buffer\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # new_td_errors = TODO...\n",
    "                replay_buffer.update_priorities(indices, new_td_errors)\n",
    "\n",
    "            # UPDATE THE TARGET Q-NETWORK #################\n",
    "\n",
    "            # Every few training steps (e.g., every 100 steps), the weights of the target network are updated with the weights of the Q-network\n",
    "\n",
    "            if iteration % target_q_network_sync_period == 0:\n",
    "                target_q_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        episode_reward_list.append(episode_reward)\n",
    "        epsilon_greedy.decay_epsilon()\n",
    "\n",
    "    return episode_reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "NUMBER_OF_TRAININGS = DEFAULT_NUMBER_OF_TRAININGS    # Change the default (global) value here if you want a specific number of trainings for this exercice\n",
    "dqn_per_trains_result_list = [[], [], []]\n",
    "\n",
    "for train_index in range(NUMBER_OF_TRAININGS):\n",
    "\n",
    "    # INSTANTIATE REQUIRED OBJECTS\n",
    "\n",
    "    q_network = QNetwork(state_dim, action_dim, nn_l1=128, nn_l2=128).to(device)\n",
    "    target_q_network = QNetwork(state_dim, action_dim, nn_l1=128, nn_l2=128).to(device) # The target Q-network is used to compute the target Q-values for the loss function\n",
    "    target_q_network.load_state_dict(q_network.state_dict()) # Initialize the target Q-network with the same weights as the Q-network\n",
    "\n",
    "    optimizer = torch.optim.AdamW(q_network.parameters(), lr=0.004, amsgrad=True)\n",
    "    #lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "    lr_scheduler = MinimumExponentialLR(optimizer, lr_decay=0.97, min_lr=0.0001)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    epsilon_greedy = EpsilonGreedy(epsilon_start=0.82, epsilon_min=0.013, epsilon_decay=0.9675, env=env, q_network=q_network)\n",
    "\n",
    "    replay_buffer = PrioritizedReplayBuffer(2000)\n",
    "\n",
    "    # TRAIN THE Q-NETWORK\n",
    "\n",
    "    episode_reward_list = train_dqn_per_agent(env,\n",
    "                                              q_network,\n",
    "                                              target_q_network,\n",
    "                                              optimizer,\n",
    "                                              loss_fn,\n",
    "                                              epsilon_greedy,\n",
    "                                              device,\n",
    "                                              lr_scheduler,\n",
    "                                              num_episodes=150,\n",
    "                                              gamma=0.9,\n",
    "                                              batch_size=128,\n",
    "                                              replay_buffer=replay_buffer,\n",
    "                                              target_q_network_sync_period=30,\n",
    "                                              alpha=0.6,\n",
    "                                              beta=0.4)\n",
    "    dqn_per_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
    "    dqn_per_trains_result_list[1].extend(episode_reward_list)\n",
    "    dqn_per_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
    "\n",
    "dqn_per_trains_result_df = pd.DataFrame(np.array(dqn_per_trains_result_list).T, columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"])\n",
    "dqn_per_trains_result_df[\"agent\"] = \"DQN 2015 + PER\"\n",
    "\n",
    "# SAVE THE ACTION-VALUE ESTIMATION FUNCTION\n",
    "\n",
    "torch.save(q_network, MODELS_DIR / \"lab1_dqn_per_q_network.pth\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", estimator=None, units=\"training_index\", data=dqn_per_trains_result_df,\n",
    "                height=7, aspect=2, alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", data=dqn_per_trains_result_df, height=7, aspect=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trains_result_df = pd.concat([naive_trains_result_df, dqn1_trains_result_df, dqn2_trains_result_df, ddqn_trains_result_df, dqn_per_trains_result_df])\n",
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", data=all_trains_result_df, height=7, aspect=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_network = torch.load(\"dqn_per_q_network.pth\").to(device)\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "test_q_network_agent(env, q_network, num_episode=3)\n",
    "\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(FIGS_DIR / \"lab1_dqn_per_tained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Experimental results\n",
    "\n",
    "**Task 6.2:** What do you observe? Why?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Dueling Double DQN : DDQN with Advantage function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dueling Double DQN (Dueling DDQN) is an enhancement over the standard DQN that aims to improve the quality of the learned value function (https://arxiv.org/abs/1511.06581). The motivation for Dueling DDQN lies in its architecture, which separately estimates two components: the value of being in a particular state (V(s)), and the advantage of taking a particular action in that state (A(s, a)).\n",
    "\n",
    "In standard DQN, a single stream of network layers estimates the Q-value directly. In contrast, Dueling DDQN has two streams to separately estimate the value and advantage functions, which are then combined to calculate the Q-value. This allows the network to more effectively learn which states are (or are not) valuable without having to learn the effect of each action for each state. This is particularly useful in environments where the value of the state does not vary much across actions.\n",
    "\n",
    "The separation of the estimation process helps in stabilizing learning and often leads to better policy evaluation, especially in cases where the action choice does not have a large impact on what happens next—making Dueling DDQN a more robust and often more efficient learning algorithm compared to the standard DQN.\n",
    "\n",
    "Thus, the Dueling DQN architecture introduces two separate streams within the neural network:\n",
    "\n",
    "   - **Value Stream (`V(s)`):** Estimates the overall value of being in a state, regardless of the action taken.\n",
    "   - **Advantage Stream (`A(s, a)`):** Estimates the relative advantage of each action in a given state.\n",
    "\n",
    "These two streams share some common layers at the beginning and then branch out. They are combined at the end to produce the Q-values using the following formula:\n",
    "\n",
    "$$Q(s, a) = V(s) + \\left( A(s, a) - \\frac{1}{|\\mathcal{A}|} \\sum_{a'} A(s, a') \\right)$$\n",
    "\n",
    "where:\n",
    "- $A(s, a)$ is the advantage of action $a$ in state $s$.\n",
    "- $|\\mathcal{A}|$ is the number of possible actions.\n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/336715783/figure/fig2/AS:816675293253632@1571721971001/Double-Dueling-Deep-Q-Learning-Network-DDDQN.jpg\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: Implement DDDQN\n",
    "\n",
    "Modify the above `QNetwork` class to implement a **Dueling Double Deep Q-Network (DDDQN)** by completing the `__init__` and `forward` methods of the new class `DuelingQNetwork` partially implemented below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the Dueling DQN network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the Q-Network\n",
    "\n",
    "**Task 7.1:** complete the `DuelingDQN` class.\n",
    "\n",
    "- **In the `__init__` Method:**\n",
    "  - Initialize the shared layers (`layer1` and `layer2`) as in the original `QNetwork`.\n",
    "  - Define two separate streams after the shared layers:\n",
    "    - **Value Stream:**\n",
    "      - A fully connected layer (`self.value_layer`) that outputs a single value.\n",
    "    - **Advantage Stream:**\n",
    "      - A fully connected layer (`self.advantage_layer`) that outputs a value for each possible action.\n",
    "\n",
    "- **In the `forward` Method:**\n",
    "  - Pass the input through the shared layers with activation functions.\n",
    "  - Pass the output of the shared layers into both the value and advantage streams.\n",
    "  - Combine the outputs of the value and advantage streams to compute the final Q-values using the formula provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingQNetwork(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A Dueling Q-Network implemented with PyTorch.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    layer1 : torch.nn.Linear\n",
    "        First fully connected layer (common feature layer).\n",
    "    layer2 : torch.nn.Linear\n",
    "        Second fully connected layer (common feature layer).\n",
    "    value_layer : torch.nn.Linear\n",
    "        Linear layer for the value stream.\n",
    "    advantage_layer : torch.nn.Linear\n",
    "        Linear layer for the advantage stream.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(x: torch.Tensor) -> torch.Tensor\n",
    "        Define the forward pass of the DuelingQNetwork.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_observations: int, n_actions: int, nn_l1: int, nn_l2: int):\n",
    "        \"\"\"\n",
    "        Initialize a new instance of DuelingQNetwork.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_observations : int\n",
    "            The size of the observation space.\n",
    "        n_actions : int\n",
    "            The size of the action space.\n",
    "        nn_l1 : int\n",
    "            The number of neurons in the first hidden layer.\n",
    "        nn_l2 : int\n",
    "            The number of neurons in the second hidden layer.\n",
    "        \"\"\"\n",
    "        super(DuelingQNetwork, self).__init__()\n",
    "\n",
    "        # Common feature layers\n",
    "        # self.layer1 = TODO...\n",
    "        # self.layer2 = TODO...\n",
    "\n",
    "        # Value stream\n",
    "        # self.value_layer = TODO...\n",
    "\n",
    "        # Advantage stream\n",
    "        # self.advantage_layer = TODO...\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Define the forward pass of the DuelingQNetwork.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            The input tensor (state).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The output tensor (Q-values).\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO...\n",
    "\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the training function\n",
    "\n",
    "**Task 7.2:** complete the `train_dddqn_agent` function (you can simply copy / paste your response from `train_ddqn_agent`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dddqn_agent(env: gym.Env,\n",
    "                      q_network: torch.nn.Module,\n",
    "                      target_q_network: torch.nn.Module,\n",
    "                      optimizer: torch.optim.Optimizer,\n",
    "                      loss_fn: Callable,\n",
    "                      epsilon_greedy: EpsilonGreedy,\n",
    "                      device: torch.device,\n",
    "                      lr_scheduler: _LRScheduler,\n",
    "                      num_episodes: int,\n",
    "                      gamma: float,\n",
    "                      batch_size: int,\n",
    "                      replay_buffer: ReplayBuffer,\n",
    "                      target_q_network_sync_period: int) -> List[float]:\n",
    "    \"\"\"\n",
    "    Train the Q-network on the given environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment to train on.\n",
    "    q_network : torch.nn.Module\n",
    "        The Q-network to train.\n",
    "    target_q_network : torch.nn.Module\n",
    "        The target Q-network to use for estimating the target Q-values.\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer to use for training.\n",
    "    loss_fn : callable\n",
    "        The loss function to use for training.\n",
    "    epsilon_greedy : EpsilonGreedy\n",
    "        The epsilon-greedy policy to use for action selection.\n",
    "    device : torch.device\n",
    "        The device to use for PyTorch computations.\n",
    "    lr_scheduler : torch.optim.lr_scheduler._LRScheduler\n",
    "        The learning rate scheduler to adjust the learning rate during training.\n",
    "    num_episodes : int\n",
    "        The number of episodes to train for.\n",
    "    gamma : float\n",
    "        The discount factor for future rewards.\n",
    "    batch_size : int\n",
    "        The size of the batch to use for training.\n",
    "    replay_buffer : ReplayBuffer\n",
    "        The replay buffer storing the experiences with their priorities.\n",
    "    target_q_network_sync_period : int\n",
    "        The number of episodes after which the target Q-network should be updated with the weights of the Q-network.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[float]\n",
    "        A list of cumulated rewards per episode.\n",
    "    \"\"\"\n",
    "    iteration = 0\n",
    "    episode_reward_list = []\n",
    "\n",
    "    for episode_index in tqdm(range(1, num_episodes)):\n",
    "        state, info = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # GET ACTION, NEXT_STATE AND REWARD ###########\n",
    "\n",
    "            action = epsilon_greedy(state)\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            replay_buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "            # UPDATE THE Q_NETWORK WEIGHTS WITH A BATCH OF EXPERIENCES FROM THE BUFFER\n",
    "\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "                # Convert to PyTorch tensors\n",
    "                batch_states_tensor = torch.tensor(batch_states, dtype=torch.float32, device=device)\n",
    "                batch_actions_tensor = torch.tensor(batch_actions, dtype=torch.long, device=device)\n",
    "                batch_rewards_tensor = torch.tensor(batch_rewards, dtype=torch.float32, device=device)\n",
    "                batch_next_states_tensor = torch.tensor(batch_next_states, dtype=torch.float32, device=device)\n",
    "                batch_dones_tensor = torch.tensor(batch_dones, dtype=torch.float32, device=device)\n",
    "\n",
    "                # Compute the target Q values for the batch\n",
    "                with torch.no_grad():\n",
    "                    # best_actions = TODO... # First, select the best action using the online network (q_network)\n",
    "\n",
    "                    # next_state_q_values = TODO... # Then, use the target network (target_q_network) to get the Q-value for these actions\n",
    "\n",
    "                    # targets = TODO... # The targets for the batch are the rewards plus the discounted maximum Q-values obtained from the next states.\n",
    "\n",
    "\n",
    "                # Compute the current Q values for the batch.\n",
    "                # \n",
    "                # The expression `gather(dim=1, index=batch_actions_tensor.unsqueeze(-1)).squeeze(-1)` is used to select specific elements from the tensor of Q-values returned by the Q-network.\n",
    "                # \n",
    "                # Here's a breakdown of the following line of code:\n",
    "                # - `q_network(batch_states_tensor)`:\n",
    "                #   This is passing a batch of states through the Q-network.\n",
    "                #   For each state, this outputs the Q-value for each possible action.\n",
    "                #   Thus, `q_network(batch_states_tensor)` returns a tensor of shape (batch_size, action_dim).\n",
    "                # \n",
    "                # - `gather(dim=1, index=batch_actions_tensor.unsqueeze(-1))`:\n",
    "                #   This is selecting the Q-values corresponding to the actions that were actually taken.\n",
    "                #   The `gather` function is used to select elements from a tensor using an index.\n",
    "                #   In this case, the index is `batch_actions_tensor.unsqueeze(-1)`, which is a tensor of the actions that were taken.\n",
    "                #   The `unsqueeze(-1)` function is used to add an extra dimension to the tensor, which is necessary for the `gather` function.\n",
    "                # \n",
    "                # - `squeeze(-1)`:\n",
    "                #   This is removing the extra dimension that was added by `unsqueeze(-1)`.\n",
    "                #   The `squeeze` function is used to remove dimensions of size 1 from a tensor.\n",
    "                #\n",
    "                # So, the entire expression is selecting the Q-values of the actions that were actually taken from the tensor of all Q-values,\n",
    "                # and returning a tensor of these selected Q-values.\n",
    "                current_q_values = q_network(batch_states_tensor).gather(dim=1, index=batch_actions_tensor.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = loss_fn(current_q_values, targets)\n",
    "\n",
    "                # Optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                lr_scheduler.step()\n",
    "\n",
    "            # UPDATE THE TARGET Q-NETWORK #################\n",
    "\n",
    "            # Every few training steps (e.g., every 100 steps), the weights of the target network are updated with the weights of the Q-network\n",
    "\n",
    "            if iteration % target_q_network_sync_period == 0:\n",
    "                target_q_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        episode_reward_list.append(episode_reward)\n",
    "        epsilon_greedy.decay_epsilon()\n",
    "\n",
    "    return episode_reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "NUMBER_OF_TRAININGS = DEFAULT_NUMBER_OF_TRAININGS    # Change the default (global) value here if you want a specific number of trainings for this exercice\n",
    "dddqn_trains_result_list = [[], [], []]\n",
    "\n",
    "for train_index in range(NUMBER_OF_TRAININGS):\n",
    "\n",
    "    # Instantiate required objects\n",
    "\n",
    "    q_network = DuelingQNetwork(state_dim, action_dim, nn_l1=128, nn_l2=128).to(device)\n",
    "    target_q_network = DuelingQNetwork(state_dim, action_dim, nn_l1=128, nn_l2=128).to(device) # The target Q-network is used to compute the target Q-values for the loss function\n",
    "    target_q_network.load_state_dict(q_network.state_dict()) # Initialize the target Q-network with the same weights as the Q-network\n",
    "\n",
    "    optimizer = torch.optim.AdamW(q_network.parameters(), lr=0.004, amsgrad=True)\n",
    "    #lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "    lr_scheduler = MinimumExponentialLR(optimizer, lr_decay=0.97, min_lr=0.0001)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    epsilon_greedy = EpsilonGreedy(epsilon_start=0.82, epsilon_min=0.013, epsilon_decay=0.9675, env=env, q_network=q_network)\n",
    "\n",
    "    replay_buffer = ReplayBuffer(2000)\n",
    "\n",
    "    # Train the q-network\n",
    "\n",
    "    episode_reward_list = train_dddqn_agent(env,\n",
    "                                            q_network,\n",
    "                                            target_q_network,\n",
    "                                            optimizer,\n",
    "                                            loss_fn,\n",
    "                                            epsilon_greedy,\n",
    "                                            device,\n",
    "                                            lr_scheduler,\n",
    "                                            num_episodes=150,\n",
    "                                            gamma=0.9,\n",
    "                                            batch_size=128,\n",
    "                                            replay_buffer=replay_buffer,\n",
    "                                            target_q_network_sync_period=30)\n",
    "    dddqn_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
    "    dddqn_trains_result_list[1].extend(episode_reward_list)\n",
    "    dddqn_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
    "\n",
    "dddqn_trains_result_df = pd.DataFrame(np.array(dddqn_trains_result_list).T, columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"])\n",
    "dddqn_trains_result_df[\"agent\"] = \"DDDQN\"\n",
    "\n",
    "# SAVE THE ACTION-VALUE ESTIMATION FUNCTION\n",
    "\n",
    "torch.save(q_network, MODELS_DIR / \"lab1_dddqn_q_network.pth\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", estimator=None, units=\"training_index\", data=dddqn_trains_result_df,\n",
    "                height=7, aspect=2, alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", data=dddqn_trains_result_df, height=7, aspect=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trains_result_df = pd.concat([naive_trains_result_df, dqn1_trains_result_df, dqn2_trains_result_df, ddqn_trains_result_df, dqn_per_trains_result_df, dddqn_trains_result_df])\n",
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", data=all_trains_result_df, height=7, aspect=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_network = torch.load(\"dddqn_q_network.pth\").to(device)\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "test_q_network_agent(env, q_network, num_episode=3)\n",
    "\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(FIGS_DIR / \"lab1_dddqn_tained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus 2: Hyperparameters optimization with Optuna\n",
    "\n",
    "Optuna is an open-source hyperparameter optimization framework designed to automate the process of searching for the best hyperparameters in machine learning models. It is highly efficient and flexible, supporting various optimization algorithms. Optuna works with Python-based machine learning libraries like PyTorch, TensorFlow, and Scikit-learn. Optuna’s core feature is its ability to perform dynamic search spaces and pruning, allowing faster convergence by terminating poorly performing trials early.\n",
    "Optuna supports distributed optimization for large-scale tuning.\n",
    "\n",
    "### Official documentation\n",
    "\n",
    "- Optuna GitHub: [https://github.com/optuna/optuna](https://github.com/optuna/optuna)\n",
    "- Optuna Documentation: [https://optuna.org](https://optuna.org)\n",
    "\n",
    "### Example of usage with PyTorch\n",
    "\n",
    "Here's an example of how to use Optuna to optimize the hyperparameters of a simple neural network with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the PyTorch model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Hyperparameters to be tuned\n",
    "    hidden_size = trial.suggest_int('hidden_size', 32, 128)\n",
    "    lr = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
    "    \n",
    "    model = Net(input_size=28*28, hidden_size=hidden_size, output_size=10)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Dummy dataset\n",
    "    X = torch.randn(100, 28*28)\n",
    "    y = torch.randint(0, 10, (100,))\n",
    "    train_loader = DataLoader(TensorDataset(X, y), batch_size=32)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(10):\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_x)\n",
    "            loss = loss_fn(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "# Optimize hyperparameters\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Show best hyperparameters\n",
    "print(study.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example creates a basic neural network and tunes the `hidden_size` and learning rate (`lr`) using Optuna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Further readings\n",
    "\n",
    "### Rainbow paper: Putting everything together\n",
    "\n",
    "In  2017, Henssel et al. performed a large experiment that combined several DQN enhancements, among them DDQN and PER (https://arxiv.org/abs/1710.02298). They found that the ehancements worked well together. The paper has become known as the Rainbow paper, since the major graph showing the cumulative performance over 57 Atrari games is multicolored.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jeremiedecock/polytechnique-csc-53439-ep-2025-students/main/assets/lab1_rainbow_curve.png?raw=true\" width=\"600px\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
